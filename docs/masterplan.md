# Execution Masterplan: Fine-Tuning Qwen3-Coder for Python Unit Test Generation

## Phase 1: Environment Setup & Data Preparation (Days 1-2)

### âœ… Step 1: Environment Setup
- [x] Install dependencies from requirements.txt (completed)
- [ ] Configure Python environment for GPU/CPU usage
- [ ] Test GPU availability and memory constraints (RTX 3050 4GB)
- [ ] Set up logging for tracking

### ðŸ”„ Step 2: Dataset Loading & Exploration
- [ ] Load CodeRM-UnitTest dataset from Hugging Face
- [ ] Explore dataset structure and understand the format
- [ ] Sample 20k records from the full 77.2k dataset
- [ ] Create 80/10/10 split (16k train / 2k val / 2k test)
- [ ] Save preprocessed splits locally

### Step 3: Data Preprocessing
- [ ] Design prompt templates for input formatting
- [ ] Tokenize data with Qwen tokenizer (max_length=2048)
- [ ] Create PyTorch datasets for training
- [ ] Implement data streaming to handle memory constraints

## Phase 2: Model Setup & Training Preparation (Days 3-4)

### Step 4: Model Loading & Configuration
- [ ] Load Qwen3-Coder-7B-Instruct with 4-bit quantization
- [ ] Configure device mapping for GPU/CPU hybrid usage
- [ ] Set up gradient checkpointing for memory efficiency
- [ ] Test model inference with sample inputs

### Step 5: QLoRA/PEFT Setup
- [ ] Configure LoRA parameters (rank 8-16, target layers)
- [ ] Set up PEFT model with appropriate adapters
- [ ] Calculate memory requirements and adjust batch sizes
- [ ] Test training setup with small batch

### Step 6: Training Configuration
- [ ] Set hyperparameters (LR 1e-5 to 2e-5, warmup 3-5%)
- [ ] Configure training arguments (micro-batch 1-2, accumulation steps)
- [ ] Set up early stopping based on validation loss
- [ ] Configure evaluation metrics and checkpointing

## Phase 3: Model Training (Days 5-7)

### Step 7: Pilot Training
- [ ] Train on subset (5k samples) for 1 epoch
- [ ] Monitor GPU memory usage and adjust if needed
- [ ] Validate training stability and convergence
- [ ] Evaluate initial results on validation set

### Step 8: Full Training
- [ ] Train on full 16k samples for 1-2 epochs
- [ ] Monitor training metrics (loss, learning rate, memory)
- [ ] Save checkpoints at regular intervals
- [ ] Apply early stopping if validation loss plateaus

## Phase 4: Evaluation & Testing (Days 8-10)

### Step 9: Model Evaluation
- [ ] Generate unit tests on test set (2k samples)
- [ ] Calculate pytest pass rate (target >80%)
- [ ] Measure code coverage using coverage.py
- [ ] Run mutation testing for robustness (target >50% mutation score)
- [ ] Benchmark inference latency (target <10s)

### Step 10: Final Validation
- [ ] Test on fresh code samples (outside dataset)
- [ ] Validate real-world usability
- [ ] Create inference pipeline for easy usage
- [ ] Document model capabilities and limitations

## Phase 5: Optimization & Deployment (Days 11-14)

### Step 11: Model Optimization
- [ ] Fine-tune hyperparameters based on results
- [ ] Experiment with different LoRA configurations
- [ ] Try alternative quantization settings if needed
- [ ] Optimize for inference speed

### Step 12: Packaging & Documentation
- [ ] Save final model and adapters
- [ ] Create inference script for local usage
- [ ] Write usage documentation
- [ ] Prepare model for sharing/deployment

## Fallback Strategies:
- **If VRAM overflow**: Reduce batch size, use CPU offload, or subset data further
- **If training unstable**: Lower learning rate, increase warmup, use gradient clipping
- **If results poor**: Try different prompt templates, increase training data, or switch to Qwen 3B

## Success Criteria Checkpoints:
- **Memory usage**: <4GB VRAM during training
- **Training stability**: Smooth loss convergence
- **Test pass rate**: >80% on validation set
- **Inference speed**: <10s per generation
- **Mutation score**: >50% robustness

## Current Status: 
**Phase 1 - Step 2 in progress**
