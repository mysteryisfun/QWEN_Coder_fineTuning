{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":12829838,"sourceType":"datasetVersion","datasetId":8113918}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"1821982e","cell_type":"markdown","source":"# Qwen2.5-Coder Fine-tuning on Kaggle P100 GPU\n\nComplete end-to-end fine-tuning pipeline for Qwen2.5-Coder-3B to generate Python unit tests.\n\n**Dataset**: https://www.kaggle.com/datasets/ujwalsr/finetuning\n**GPU**: Kaggle P100 (16GB VRAM)\n**Method**: LoRA fine-tuning with 4-bit quantization\n\n---","metadata":{}},{"id":"225ad047","cell_type":"markdown","source":"## 1. Import Required Libraries\n\nInstalling and importing all necessary dependencies for training.","metadata":{}},{"id":"3223b341-b51b-47c9-8e72-fde33c7243f8","cell_type":"code","source":"# Install required packages\n!pip install -q transformers>=4.36.0\n!pip install -q peft>=0.7.0\n!pip install -q bitsandbytes>=0.41.0\n!pip install -q accelerate>=0.24.0\n!pip install -q datasets\n!pip install -q tqdm\n\nprint(\"‚úÖ Packages installed successfully!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-21T18:10:20.842833Z","iopub.execute_input":"2025-08-21T18:10:20.843670Z","iopub.status.idle":"2025-08-21T18:11:47.985520Z","shell.execute_reply.started":"2025-08-21T18:10:20.843642Z","shell.execute_reply":"2025-08-21T18:11:47.984623Z"}},"outputs":[{"name":"stdout","text":"‚úÖ Packages installed successfully!\n","output_type":"stream"}],"execution_count":3},{"id":"f8fd43e8","cell_type":"code","source":"# Import essential libraries\nimport os\nimport sys\nimport pickle\nimport torch\nimport torch.nn as nn\nimport gc\nimport time\nimport json\nfrom pathlib import Path\nfrom typing import Dict, List, Any, Optional\nfrom dataclasses import dataclass\nimport numpy as np\nimport pandas as pd\n\n# Transformers and PEFT\nfrom transformers import (\n    AutoTokenizer, AutoModelForCausalLM, \n    TrainingArguments, Trainer,\n    BitsAndBytesConfig, DataCollatorForLanguageModeling\n)\nfrom peft import LoraConfig, get_peft_model, TaskType\nfrom torch.utils.data import Dataset, DataLoader\nfrom tqdm import tqdm\n\n# Check GPU availability\nprint(f\"üñ•Ô∏è  CUDA available: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    print(f\"   GPU: {torch.cuda.get_device_name()}\")\n    print(f\"   GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f}GB\")\n    \n# Set random seeds for reproducibility\ntorch.manual_seed(42)\nnp.random.seed(42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-21T18:16:29.945587Z","iopub.execute_input":"2025-08-21T18:16:29.945867Z","iopub.status.idle":"2025-08-21T18:16:39.384992Z","shell.execute_reply.started":"2025-08-21T18:16:29.945844Z","shell.execute_reply":"2025-08-21T18:16:39.384336Z"}},"outputs":[{"name":"stderr","text":"2025-08-21 18:16:36.061967: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1755800196.084473     201 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1755800196.091188     201 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"üñ•Ô∏è  CUDA available: True\n   GPU: Tesla P100-PCIE-16GB\n   GPU Memory: 15.9GB\n","output_type":"stream"}],"execution_count":1},{"id":"2b2eb68b","cell_type":"markdown","source":"## 2. Load and Prepare Dataset\n\nLoading the three pickle files from the Kaggle dataset and examining the data structure.","metadata":{}},{"id":"f8481b33","cell_type":"code","source":"# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Dataset paths (Kaggle input directory)\ndataset_dir = \"/kaggle/input/finetuning\"\n\ntrain_pkl_path = f\"{dataset_dir}/train_split.pkl\"\nval_pkl_path = f\"{dataset_dir}/val_split.pkl\"\ntest_pkl_path = f\"{dataset_dir}/test_split.pkl\"\n\n# Verify files exist\nprint(f\"\\nüîç Checking for dataset files in: {dataset_dir}\")\nfor path in [train_pkl_path, val_pkl_path, test_pkl_path]:\n    if os.path.exists(path):\n        file_size = os.path.getsize(path) / 1024 / 1024  # Size in MB\n        print(f\"‚úÖ Found: {path} ({file_size:.1f} MB)\")\n    else:\n        print(f\"‚ùå Missing: {path}\")\n        \n# List all files in dataset directory\nif os.path.exists(dataset_dir):\n    print(f\"\\nüìÅ Dataset directory contents:\")\n    for file in os.listdir(dataset_dir):\n        file_path = os.path.join(dataset_dir, file)\n        if os.path.isfile(file_path):\n            file_size = os.path.getsize(file_path) / 1024 / 1024\n            print(f\"   - {file} ({file_size:.1f} MB)\")\n        else:\n            print(f\"   - {file}/ (directory)\")\nelse:\n    print(f\"‚ùå Dataset directory not found: {dataset_dir}\")\n    print(\"üìã Available input directories:\")\n    for item in os.listdir(\"/kaggle/input\"):\n        print(f\"   - /kaggle/input/{item}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-21T18:16:46.168582Z","iopub.execute_input":"2025-08-21T18:16:46.169257Z","iopub.status.idle":"2025-08-21T18:16:46.182125Z","shell.execute_reply.started":"2025-08-21T18:16:46.169225Z","shell.execute_reply":"2025-08-21T18:16:46.181358Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/finetuning/train_split.pkl\n/kaggle/input/finetuning/val_split.pkl\n/kaggle/input/finetuning/test_split.pkl\n\nüîç Checking for dataset files in: /kaggle/input/finetuning\n‚úÖ Found: /kaggle/input/finetuning/train_split.pkl (2198.0 MB)\n‚úÖ Found: /kaggle/input/finetuning/val_split.pkl (274.8 MB)\n‚úÖ Found: /kaggle/input/finetuning/test_split.pkl (273.7 MB)\n\nüìÅ Dataset directory contents:\n   - train_split.pkl (2198.0 MB)\n   - val_split.pkl (274.8 MB)\n   - test_split.pkl (273.7 MB)\n","output_type":"stream"}],"execution_count":2},{"id":"b2522788","cell_type":"code","source":"# Load pickle datasets\nprint(\"üìÇ Loading pickle datasets...\")\n\nwith open(train_pkl_path, 'rb') as f:\n    train_data = pickle.load(f)\n    \nwith open(val_pkl_path, 'rb') as f:\n    val_data = pickle.load(f)\n    \nwith open(test_pkl_path, 'rb') as f:\n    test_data = pickle.load(f)\n\nprint(f\"‚úÖ Datasets loaded successfully:\")\nprint(f\"   - Training: {len(train_data):,} samples\")\nprint(f\"   - Validation: {len(val_data):,} samples\")\nprint(f\"   - Test: {len(test_data):,} samples\")\nprint(f\"   - Total: {len(train_data) + len(val_data) + len(test_data):,} samples\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-21T18:16:49.245991Z","iopub.execute_input":"2025-08-21T18:16:49.246381Z","iopub.status.idle":"2025-08-21T18:16:52.855420Z","shell.execute_reply.started":"2025-08-21T18:16:49.246345Z","shell.execute_reply":"2025-08-21T18:16:52.854690Z"}},"outputs":[{"name":"stdout","text":"üìÇ Loading pickle datasets...\n‚úÖ Datasets loaded successfully:\n   - Training: 14,049 samples\n   - Validation: 1,756 samples\n   - Test: 1,757 samples\n   - Total: 17,562 samples\n","output_type":"stream"}],"execution_count":3},{"id":"5770d877","cell_type":"code","source":"# Examine data structure\nprint(\"üîç Examining data structure...\")\n\nif len(train_data) > 0:\n    sample = train_data[0]\n    print(f\"\\nüìã Sample keys: {list(sample.keys())}\")\n    \n    # Check if data is pre-tokenized or raw text\n    if 'input_ids' in sample:\n        print(\"‚úÖ Data is pre-tokenized\")\n        print(f\"   - Input IDs length: {len(sample['input_ids'])}\")\n        print(f\"   - Has attention mask: {'attention_mask' in sample}\")\n        print(f\"   - Has labels: {'labels' in sample}\")\n    else:\n        print(\"üìù Data contains raw text\")\n        for key in sample.keys():\n            if isinstance(sample[key], str):\n                print(f\"   - {key}: {len(sample[key])} characters\")\n                print(f\"     Preview: {sample[key][:100]}...\")\n            else:\n                print(f\"   - {key}: {type(sample[key])}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-21T18:16:55.465921Z","iopub.execute_input":"2025-08-21T18:16:55.466255Z","iopub.status.idle":"2025-08-21T18:16:55.472351Z","shell.execute_reply.started":"2025-08-21T18:16:55.466230Z","shell.execute_reply":"2025-08-21T18:16:55.471471Z"}},"outputs":[{"name":"stdout","text":"üîç Examining data structure...\n\nüìã Sample keys: ['task_id', 'question', 'code_ground_truth', 'code_generate', 'unit_tests']\nüìù Data contains raw text\n   - task_id: <class 'int'>\n   - question: 1355 characters\n     Preview: As AtCoder Beginner Contest 100 is taking place, the office of AtCoder, Inc. is decorated with a seq...\n   - code_ground_truth: 147 characters\n     Preview: def max_operations_on_sequence(N, a):\n    ans = 0\n    for i in a:\n        while i % 2 == 0:\n        ...\n   - code_generate: 10335 characters\n     Preview: [{\"sol_id\": 0, \"code\": \"def max_operations_on_sequence(N, a):\\n    \\\"\\\"\\\"\\n    Calculate the maximum...\n   - unit_tests: 133288 characters\n     Preview: [{\"ut_id\": 0, \"code\": \"import unittest\\n\\nclass TestMaxOperationsOnSequence(unittest.TestCase):\\n\\n ...\n","output_type":"stream"}],"execution_count":4},{"id":"5ff68ef8","cell_type":"markdown","source":"## 3. Data Preprocessing and Feature Engineering\n\nSetting up the PyTorch Dataset class and data preprocessing pipeline.","metadata":{}},{"id":"814ed59f","cell_type":"code","source":"# Training configuration for Kaggle P100\n@dataclass\nclass KaggleTrainingConfig:\n    \"\"\"Optimized configuration for Kaggle P100 GPU (16GB VRAM)\"\"\"\n    \n    # Model configuration\n    model_name: str = \"Qwen/Qwen2.5-Coder-3B-Instruct\"\n    max_length: int = 512  # P100 can handle longer sequences\n    \n    # Training parameters - P100 optimized\n    train_batch_size: int = 1\n    eval_batch_size: int = 2\n    gradient_accumulation_steps: int = 16  # Effective batch size: 16\n    num_epochs: int = 3\n    learning_rate: float = 2e-4\n    weight_decay: float = 0.001\n    warmup_ratio: float = 0.03\n    \n    # LoRA configuration\n    lora_r: int = 8\n    lora_alpha: int = 16\n    lora_dropout: float = 0.05\n    target_modules: List[str] = None\n    \n    # Output configuration\n    output_dir: str = \"/kaggle/working/qwen-coder-finetune\"\n    run_name: str = \"qwen-coder-unittest-kaggle\"\n    logging_steps: int = 10\n    save_steps: int = 500\n    eval_steps: int = 500\n    \n    # Hardware optimization for P100\n    use_cuda: bool = True\n    mixed_precision: bool = True\n    gradient_checkpointing: bool = True  # Enable for P100\n    \n    def __post_init__(self):\n        if self.target_modules is None:\n            self.target_modules = [\n                \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n                \"gate_proj\", \"up_proj\", \"down_proj\"\n            ]\n\n# Initialize configuration\nconfig = KaggleTrainingConfig()\nprint(f\"üìã Training Configuration:\")\nprint(f\"   - Model: {config.model_name}\")\nprint(f\"   - Epochs: {config.num_epochs}\")\nprint(f\"   - Batch size: {config.train_batch_size}\")\nprint(f\"   - Effective batch size: {config.train_batch_size * config.gradient_accumulation_steps}\")\nprint(f\"   - Learning rate: {config.learning_rate}\")\nprint(f\"   - Max length: {config.max_length}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-21T18:41:32.805054Z","iopub.execute_input":"2025-08-21T18:41:32.805718Z","iopub.status.idle":"2025-08-21T18:41:32.815337Z","shell.execute_reply.started":"2025-08-21T18:41:32.805692Z","shell.execute_reply":"2025-08-21T18:41:32.814507Z"}},"outputs":[{"name":"stdout","text":"üìã Training Configuration:\n   - Model: Qwen/Qwen2.5-Coder-3B-Instruct\n   - Epochs: 3\n   - Batch size: 1\n   - Effective batch size: 16\n   - Learning rate: 0.0002\n   - Max length: 512\n","output_type":"stream"}],"execution_count":55},{"id":"c2391cdf","cell_type":"code","source":"# PyTorch Dataset class for unit test generation\nclass UnitTestDataset(Dataset):\n    \"\"\"PyTorch Dataset for unit test generation training\"\"\"\n    \n    def __init__(self, data: List[Dict], tokenizer, max_length: int = 1024):\n        self.data = data\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n        \n        print(f\"Dataset initialized with {len(data):,} samples\")\n        if len(data) > 0:\n            print(f\"Sample keys: {list(data[0].keys())}\")\n    \n    def __len__(self) -> int:\n        return len(self.data)\n    \n    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:\n        item = self.data[idx]\n        \n        # Handle pre-tokenized data\n        if 'input_ids' in item and 'labels' in item:\n            return {\n                'input_ids': torch.tensor(item['input_ids'][:self.max_length], dtype=torch.long),\n                'attention_mask': torch.tensor(item['attention_mask'][:self.max_length], dtype=torch.long),\n                'labels': torch.tensor(item['labels'][:self.max_length], dtype=torch.long)\n            }\n        \n        # Handle raw text data\n        else:\n            code = item.get('code', '')\n            unit_test = item.get('unit_test', '')\n            \n            # Create training prompt\n            prompt = f\"# Generate a unit test for the following Python function:\\n{code}\\n\\n# Unit test:\\n{unit_test}\"\n            \n            # Tokenize\n            encoded = self.tokenizer(\n                prompt,\n                truncation=True,\n                padding='max_length',\n                max_length=self.max_length,\n                return_tensors='pt'\n            )\n            \n            return {\n                'input_ids': encoded['input_ids'].squeeze(),\n                'attention_mask': encoded['attention_mask'].squeeze(),\n                'labels': encoded['input_ids'].squeeze().clone()\n            }\n\nprint(\"‚úÖ UnitTestDataset class defined\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-21T18:41:38.724683Z","iopub.execute_input":"2025-08-21T18:41:38.725367Z","iopub.status.idle":"2025-08-21T18:41:38.733444Z","shell.execute_reply.started":"2025-08-21T18:41:38.725340Z","shell.execute_reply":"2025-08-21T18:41:38.732583Z"}},"outputs":[{"name":"stdout","text":"‚úÖ UnitTestDataset class defined\n","output_type":"stream"}],"execution_count":56},{"id":"4fa73ebb","cell_type":"markdown","source":"## 4. Model Architecture Setup\n\nLoading the Qwen2.5-Coder model with quantization and setting up LoRA fine-tuning.","metadata":{}},{"id":"c058c5bd","cell_type":"code","source":"# Utility functions\ndef clear_memory():\n    \"\"\"Clear GPU and system memory\"\"\"\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n        torch.cuda.synchronize()\n    gc.collect()\n\ndef check_gpu_memory():\n    \"\"\"Check and print GPU memory usage\"\"\"\n    if torch.cuda.is_available():\n        allocated = torch.cuda.memory_allocated() / 1024**3\n        reserved = torch.cuda.memory_reserved() / 1024**3\n        print(f\"üñ•Ô∏è  GPU Memory - Allocated: {allocated:.2f}GB, Reserved: {reserved:.2f}GB\")\n    else:\n        print(\"‚ùå CUDA not available\")\n\ndef create_quantization_config() -> BitsAndBytesConfig:\n    \"\"\"Create 4-bit quantization configuration\"\"\"\n    return BitsAndBytesConfig(\n        load_in_4bit=True,\n        bnb_4bit_use_double_quant=True,\n        bnb_4bit_quant_type=\"nf4\",\n        bnb_4bit_compute_dtype=torch.bfloat16\n    )\n\nprint(\"‚úÖ Utility functions defined\")\nclear_memory()\ncheck_gpu_memory()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-21T18:41:42.084681Z","iopub.execute_input":"2025-08-21T18:41:42.085254Z","iopub.status.idle":"2025-08-21T18:41:42.485657Z","shell.execute_reply.started":"2025-08-21T18:41:42.085230Z","shell.execute_reply":"2025-08-21T18:41:42.484881Z"}},"outputs":[{"name":"stdout","text":"‚úÖ Utility functions defined\nüñ•Ô∏è  GPU Memory - Allocated: 2.04GB, Reserved: 8.45GB\n","output_type":"stream"}],"execution_count":57},{"id":"2458f78a","cell_type":"code","source":"# Load tokenizer\nprint(\"ü§ñ Loading tokenizer...\")\n\ntokenizer = AutoTokenizer.from_pretrained(\n    config.model_name,\n    trust_remote_code=True,\n    padding_side=\"left\"\n)\n\n# Add pad token if missing\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\n    print(\"‚úÖ Added pad token\")\n\nprint(f\"‚úÖ Tokenizer loaded: {config.model_name}\")\nprint(f\"   - Vocab size: {len(tokenizer)}\")\nprint(f\"   - Special tokens: pad={tokenizer.pad_token}, eos={tokenizer.eos_token}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-21T18:17:09.885516Z","iopub.execute_input":"2025-08-21T18:17:09.886298Z","iopub.status.idle":"2025-08-21T18:17:10.652933Z","shell.execute_reply.started":"2025-08-21T18:17:09.886269Z","shell.execute_reply":"2025-08-21T18:17:10.652080Z"}},"outputs":[{"name":"stdout","text":"ü§ñ Loading tokenizer...\n‚úÖ Tokenizer loaded: Qwen/Qwen2.5-Coder-3B-Instruct\n   - Vocab size: 151665\n   - Special tokens: pad=<|endoftext|>, eos=<|im_end|>\n","output_type":"stream"}],"execution_count":8},{"id":"fbd0ff31-9582-44d7-a8e3-1e08ec8eeb0c","cell_type":"code","source":"# Fix bitsandbytes installation for Kaggle\n!pip install -U bitsandbytes>=0.41.0\n!pip install -U accelerate>=0.24.0\n\n# Restart kernel after installation\nimport os\nos._exit(00)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-21T18:16:09.924683Z","iopub.execute_input":"2025-08-21T18:16:09.924961Z","execution_failed":"2025-08-21T18:16:18.112Z"}},"outputs":[],"execution_count":null},{"id":"294df842","cell_type":"code","source":"# Load model with quantization\nprint(\"ü§ñ Loading model with quantization...\")\n\nquantization_config = create_quantization_config()\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    config.model_name,\n    quantization_config=quantization_config,\n    device_map=\"auto\",\n    trust_remote_code=True,\n    torch_dtype=torch.bfloat16,\n    attn_implementation=\"eager\"  # Avoid flash attention compatibility issues\n)\n\n# Resize token embeddings if needed\nif len(tokenizer) != model.config.vocab_size:\n    model.resize_token_embeddings(len(tokenizer))\n    print(f\"   - Resized token embeddings to {len(tokenizer)}\")\n\nprint(f\"‚úÖ Model loaded: {config.model_name}\")\nprint(f\"   - Parameters: {model.num_parameters():,}\")\nprint(f\"   - Vocab size: {model.config.vocab_size}\")\ncheck_gpu_memory()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-21T18:41:48.744677Z","iopub.execute_input":"2025-08-21T18:41:48.745447Z","iopub.status.idle":"2025-08-21T18:41:54.373180Z","shell.execute_reply.started":"2025-08-21T18:41:48.745415Z","shell.execute_reply":"2025-08-21T18:41:54.372501Z"}},"outputs":[{"name":"stdout","text":"ü§ñ Loading model with quantization...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eb059cca82c04673b572d54dba82a113"}},"metadata":{}},{"name":"stdout","text":"   - Resized token embeddings to 151665\n‚úÖ Model loaded: Qwen/Qwen2.5-Coder-3B-Instruct\n   - Parameters: 3,085,383,680\n   - Vocab size: 151665\nüñ•Ô∏è  GPU Memory - Allocated: 3.96GB, Reserved: 8.45GB\n","output_type":"stream"}],"execution_count":58},{"id":"bc0204fa","cell_type":"code","source":"# Setup LoRA fine-tuning\nprint(\"üîß Setting up LoRA configuration...\")\n\nlora_config = LoraConfig(\n    r=config.lora_r,\n    lora_alpha=config.lora_alpha,\n    target_modules=config.target_modules,\n    lora_dropout=config.lora_dropout,\n    bias=\"none\",\n    task_type=TaskType.CAUSAL_LM,\n)\n\nmodel = get_peft_model(model, lora_config)\n\n# Print trainable parameters\ntrainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\ntotal_params = sum(p.numel() for p in model.parameters())\n\nprint(f\"‚úÖ LoRA setup complete:\")\nprint(f\"   - Trainable params: {trainable_params:,}\")\nprint(f\"   - Total params: {total_params:,}\")\nprint(f\"   - Trainable percentage: {100 * trainable_params / total_params:.2f}%\")\n\ncheck_gpu_memory()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-21T18:41:55.904526Z","iopub.execute_input":"2025-08-21T18:41:55.905137Z","iopub.status.idle":"2025-08-21T18:41:56.323248Z","shell.execute_reply.started":"2025-08-21T18:41:55.905115Z","shell.execute_reply":"2025-08-21T18:41:56.322530Z"}},"outputs":[{"name":"stdout","text":"üîß Setting up LoRA configuration...\n‚úÖ LoRA setup complete:\n   - Trainable params: 14,966,784\n   - Total params: 1,713,084,416\n   - Trainable percentage: 0.87%\nüñ•Ô∏è  GPU Memory - Allocated: 4.02GB, Reserved: 8.45GB\n","output_type":"stream"}],"execution_count":59},{"id":"69837aef","cell_type":"markdown","source":"## 5. Training Configuration and Hyperparameters\n\nSetting up training arguments and creating PyTorch datasets.","metadata":{}},{"id":"d47688a7","cell_type":"code","source":"# Create PyTorch datasets\nprint(\"üìä Creating PyTorch datasets...\")\n\ntrain_dataset = UnitTestDataset(train_data, tokenizer, config.max_length)\nval_dataset = UnitTestDataset(val_data, tokenizer, config.max_length)\ntest_dataset = UnitTestDataset(test_data, tokenizer, config.max_length)\n\nprint(f\"‚úÖ Datasets created:\")\nprint(f\"   - Training: {len(train_dataset):,} samples\")\nprint(f\"   - Validation: {len(val_dataset):,} samples\")\nprint(f\"   - Test: {len(test_dataset):,} samples\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-21T18:42:02.704619Z","iopub.execute_input":"2025-08-21T18:42:02.705189Z","iopub.status.idle":"2025-08-21T18:42:02.710274Z","shell.execute_reply.started":"2025-08-21T18:42:02.705154Z","shell.execute_reply":"2025-08-21T18:42:02.709430Z"}},"outputs":[{"name":"stdout","text":"üìä Creating PyTorch datasets...\nDataset initialized with 14,049 samples\nSample keys: ['task_id', 'question', 'code_ground_truth', 'code_generate', 'unit_tests']\nDataset initialized with 1,756 samples\nSample keys: ['task_id', 'question', 'code_ground_truth', 'code_generate', 'unit_tests']\nDataset initialized with 1,757 samples\nSample keys: ['task_id', 'question', 'code_ground_truth', 'code_generate', 'unit_tests']\n‚úÖ Datasets created:\n   - Training: 14,049 samples\n   - Validation: 1,756 samples\n   - Test: 1,757 samples\n","output_type":"stream"}],"execution_count":60},{"id":"2ccfe6b6","cell_type":"code","source":"# Create data collator\ndata_collator = DataCollatorForLanguageModeling(\n    tokenizer=tokenizer,\n    mlm=False,\n    pad_to_multiple_of=8\n)\n\nprint(\"‚úÖ Data collator created\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-21T18:42:06.104243Z","iopub.execute_input":"2025-08-21T18:42:06.104874Z","iopub.status.idle":"2025-08-21T18:42:06.109115Z","shell.execute_reply.started":"2025-08-21T18:42:06.104850Z","shell.execute_reply":"2025-08-21T18:42:06.108252Z"}},"outputs":[{"name":"stdout","text":"‚úÖ Data collator created\n","output_type":"stream"}],"execution_count":61},{"id":"5d883b3d","cell_type":"code","source":"# Create training arguments\ntraining_args = TrainingArguments(\n    output_dir=config.output_dir,\n    run_name=config.run_name,\n    \n    # Training parameters\n    num_train_epochs=config.num_epochs,\n    per_device_train_batch_size=config.train_batch_size,\n    per_device_eval_batch_size=config.eval_batch_size,\n    gradient_accumulation_steps=config.gradient_accumulation_steps,\n    \n    # Optimization\n    learning_rate=config.learning_rate,\n    weight_decay=config.weight_decay,\n    warmup_ratio=config.warmup_ratio,\n    \n    # Hardware optimization for P100\n    fp16=True,\n    bf16=False,\n    gradient_checkpointing=False,\n    dataloader_pin_memory=False,\n    \n    # Logging and saving\n    logging_steps=config.logging_steps,\n    eval_steps=config.eval_steps,\n    save_steps=config.save_steps,\n    eval_strategy=\"steps\",\n    save_strategy=\"steps\",\n    \n    # Model selection\n    remove_unused_columns=False,\n    load_best_model_at_end=True,\n    metric_for_best_model=\"eval_loss\",\n    greater_is_better=False,\n    \n    # Disable external logging\n    report_to=[]\n)\n\nprint(\"‚úÖ Training arguments created\")\nprint(f\"   - Total training steps: {len(train_dataset) // (config.train_batch_size * config.gradient_accumulation_steps) * config.num_epochs}\")\nprint(f\"   - Steps per epoch: {len(train_dataset) // (config.train_batch_size * config.gradient_accumulation_steps)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-21T18:42:08.264924Z","iopub.execute_input":"2025-08-21T18:42:08.265512Z","iopub.status.idle":"2025-08-21T18:42:08.294988Z","shell.execute_reply.started":"2025-08-21T18:42:08.265487Z","shell.execute_reply":"2025-08-21T18:42:08.294360Z"}},"outputs":[{"name":"stdout","text":"‚úÖ Training arguments created\n   - Total training steps: 2634\n   - Steps per epoch: 878\n","output_type":"stream"}],"execution_count":62},{"id":"ad8b6875-6f3f-439a-82a1-f4e2eafe1aaa","cell_type":"code","source":"# CRITICAL FIX: Enable gradients for LoRA parameters\nprint(\"üîß Fixing LoRA parameter gradients...\")\n\n# Method 1: Enable gradients for all LoRA parameters\nfor name, param in model.named_parameters():\n    if any(keyword in name for keyword in ['lora_A', 'lora_B', 'lora_embedding']):\n        param.requires_grad = True\n        print(f\"‚úÖ Enabled: {name}\")\n\n# Method 2: Alternative - enable all trainable parameters\nmodel.train()\nfor param in model.parameters():\n    if param.requires_grad:\n        param.requires_grad = True\n\n# Verify fix\ntrainable_count = sum(1 for p in model.parameters() if p.requires_grad)\ntotal_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n\nprint(f\"üìä Verification:\")\nprint(f\"   - Trainable parameter tensors: {trainable_count}\")\nprint(f\"   - Trainable parameters: {total_params:,}\")\n\nif trainable_count == 0:\n    print(\"‚ùå STILL NO TRAINABLE PARAMETERS!\")\n    # Force enable LoRA\n    model.enable_adapters()\n    print(\"üîÑ Forced LoRA adapter activation\")\nelse:\n    print(\"‚úÖ Ready for training!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-21T18:42:15.145495Z","iopub.execute_input":"2025-08-21T18:42:15.146267Z","iopub.status.idle":"2025-08-21T18:42:15.194358Z","shell.execute_reply.started":"2025-08-21T18:42:15.146245Z","shell.execute_reply":"2025-08-21T18:42:15.193540Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"üîß Fixing LoRA parameter gradients...\n‚úÖ Enabled: base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.0.mlp.gate_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.0.mlp.gate_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.0.mlp.up_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.0.mlp.up_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.0.mlp.down_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.0.mlp.down_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.1.self_attn.k_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.1.self_attn.k_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.1.self_attn.o_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.1.self_attn.o_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.1.mlp.gate_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.1.mlp.gate_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.1.mlp.up_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.1.mlp.up_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.1.mlp.down_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.1.mlp.down_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.2.self_attn.k_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.2.self_attn.k_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.2.self_attn.o_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.2.self_attn.o_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.2.mlp.gate_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.2.mlp.gate_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.2.mlp.up_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.2.mlp.up_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.2.mlp.down_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.2.mlp.down_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.3.self_attn.k_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.3.self_attn.k_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.3.self_attn.o_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.3.self_attn.o_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.3.mlp.gate_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.3.mlp.gate_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.3.mlp.up_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.3.mlp.up_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.3.mlp.down_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.3.mlp.down_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.4.self_attn.k_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.4.self_attn.k_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.4.self_attn.o_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.4.self_attn.o_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.4.mlp.gate_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.4.mlp.gate_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.4.mlp.up_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.4.mlp.up_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.4.mlp.down_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.4.mlp.down_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.5.self_attn.k_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.5.self_attn.k_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.5.self_attn.o_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.5.self_attn.o_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.5.mlp.gate_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.5.mlp.gate_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.5.mlp.up_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.5.mlp.up_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.5.mlp.down_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.5.mlp.down_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.6.self_attn.k_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.6.self_attn.k_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.6.self_attn.o_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.6.self_attn.o_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.6.mlp.gate_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.6.mlp.gate_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.6.mlp.up_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.6.mlp.up_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.6.mlp.down_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.6.mlp.down_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.7.self_attn.k_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.7.self_attn.k_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.7.self_attn.o_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.7.self_attn.o_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.7.mlp.gate_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.7.mlp.gate_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.7.mlp.up_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.7.mlp.up_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.7.mlp.down_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.7.mlp.down_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.8.self_attn.k_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.8.self_attn.k_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.8.self_attn.o_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.8.self_attn.o_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.8.mlp.gate_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.8.mlp.gate_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.8.mlp.up_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.8.mlp.up_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.8.mlp.down_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.8.mlp.down_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.9.self_attn.k_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.9.self_attn.k_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.9.self_attn.o_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.9.self_attn.o_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.9.mlp.gate_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.9.mlp.gate_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.9.mlp.up_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.9.mlp.up_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.9.mlp.down_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.9.mlp.down_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.10.self_attn.k_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.10.self_attn.k_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.10.self_attn.o_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.10.self_attn.o_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.10.mlp.gate_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.10.mlp.gate_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.10.mlp.up_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.10.mlp.up_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.10.mlp.down_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.10.mlp.down_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.11.self_attn.k_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.11.self_attn.k_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.11.self_attn.o_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.11.self_attn.o_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.11.mlp.gate_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.11.mlp.gate_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.11.mlp.up_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.11.mlp.up_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.11.mlp.down_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.11.mlp.down_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.12.self_attn.k_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.12.self_attn.k_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.12.self_attn.o_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.12.self_attn.o_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.12.mlp.gate_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.12.mlp.gate_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.12.mlp.up_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.12.mlp.up_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.12.mlp.down_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.12.mlp.down_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.13.self_attn.k_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.13.self_attn.k_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.13.self_attn.o_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.13.self_attn.o_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.13.mlp.gate_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.13.mlp.gate_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.13.mlp.up_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.13.mlp.up_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.13.mlp.down_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.13.mlp.down_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.14.self_attn.k_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.14.self_attn.k_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.14.self_attn.o_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.14.self_attn.o_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.14.mlp.gate_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.14.mlp.gate_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.14.mlp.up_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.14.mlp.up_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.14.mlp.down_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.14.mlp.down_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.15.self_attn.k_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.15.self_attn.k_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.15.self_attn.o_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.15.self_attn.o_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.15.mlp.gate_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.15.mlp.gate_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.15.mlp.up_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.15.mlp.up_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.15.mlp.down_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.15.mlp.down_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.16.self_attn.k_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.16.self_attn.k_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.16.self_attn.o_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.16.self_attn.o_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.16.mlp.gate_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.16.mlp.gate_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.16.mlp.up_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.16.mlp.up_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.16.mlp.down_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.16.mlp.down_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.17.self_attn.k_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.17.self_attn.k_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.17.self_attn.o_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.17.self_attn.o_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.17.mlp.gate_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.17.mlp.gate_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.17.mlp.up_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.17.mlp.up_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.17.mlp.down_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.17.mlp.down_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.18.self_attn.k_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.18.self_attn.k_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.18.self_attn.o_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.18.self_attn.o_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.18.mlp.gate_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.18.mlp.gate_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.18.mlp.up_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.18.mlp.up_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.18.mlp.down_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.18.mlp.down_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.19.self_attn.k_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.19.self_attn.k_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.19.self_attn.o_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.19.self_attn.o_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.19.mlp.gate_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.19.mlp.gate_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.19.mlp.up_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.19.mlp.up_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.19.mlp.down_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.19.mlp.down_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.20.self_attn.k_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.20.self_attn.k_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.20.self_attn.o_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.20.self_attn.o_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.20.mlp.gate_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.20.mlp.gate_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.20.mlp.up_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.20.mlp.up_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.20.mlp.down_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.20.mlp.down_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.21.self_attn.k_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.21.self_attn.k_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.21.self_attn.o_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.21.self_attn.o_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.21.mlp.gate_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.21.mlp.gate_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.21.mlp.up_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.21.mlp.up_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.21.mlp.down_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.21.mlp.down_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.22.self_attn.k_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.22.self_attn.k_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.22.self_attn.o_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.22.self_attn.o_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.22.mlp.gate_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.22.mlp.gate_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.22.mlp.up_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.22.mlp.up_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.22.mlp.down_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.22.mlp.down_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.23.self_attn.k_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.23.self_attn.k_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.23.self_attn.o_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.23.self_attn.o_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.23.mlp.gate_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.23.mlp.gate_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.23.mlp.up_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.23.mlp.up_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.23.mlp.down_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.23.mlp.down_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.24.self_attn.q_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.24.self_attn.q_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.24.self_attn.k_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.24.self_attn.k_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.24.self_attn.v_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.24.self_attn.v_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.24.self_attn.o_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.24.self_attn.o_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.24.mlp.gate_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.24.mlp.gate_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.24.mlp.up_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.24.mlp.up_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.24.mlp.down_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.24.mlp.down_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.25.self_attn.q_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.25.self_attn.q_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.25.self_attn.k_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.25.self_attn.k_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.25.self_attn.v_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.25.self_attn.v_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.25.self_attn.o_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.25.self_attn.o_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.25.mlp.gate_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.25.mlp.gate_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.25.mlp.up_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.25.mlp.up_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.25.mlp.down_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.25.mlp.down_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.26.self_attn.q_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.26.self_attn.q_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.26.self_attn.k_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.26.self_attn.k_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.26.self_attn.v_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.26.self_attn.v_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.26.self_attn.o_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.26.self_attn.o_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.26.mlp.gate_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.26.mlp.gate_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.26.mlp.up_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.26.mlp.up_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.26.mlp.down_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.26.mlp.down_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.27.self_attn.q_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.27.self_attn.q_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.27.self_attn.k_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.27.self_attn.k_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.27.self_attn.v_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.27.self_attn.v_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.27.self_attn.o_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.27.self_attn.o_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.27.mlp.gate_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.27.mlp.gate_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.27.mlp.up_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.27.mlp.up_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.27.mlp.down_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.27.mlp.down_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.28.self_attn.q_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.28.self_attn.q_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.28.self_attn.k_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.28.self_attn.k_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.28.self_attn.v_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.28.self_attn.v_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.28.self_attn.o_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.28.self_attn.o_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.28.mlp.gate_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.28.mlp.gate_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.28.mlp.up_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.28.mlp.up_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.28.mlp.down_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.28.mlp.down_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.29.self_attn.q_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.29.self_attn.q_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.29.self_attn.k_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.29.self_attn.k_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.29.self_attn.v_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.29.self_attn.v_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.29.self_attn.o_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.29.self_attn.o_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.29.mlp.gate_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.29.mlp.gate_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.29.mlp.up_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.29.mlp.up_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.29.mlp.down_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.29.mlp.down_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.30.self_attn.q_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.30.self_attn.q_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.30.self_attn.k_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.30.self_attn.k_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.30.self_attn.v_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.30.self_attn.v_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.30.self_attn.o_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.30.self_attn.o_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.30.mlp.gate_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.30.mlp.gate_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.30.mlp.up_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.30.mlp.up_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.30.mlp.down_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.30.mlp.down_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.31.self_attn.q_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.31.self_attn.q_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.31.self_attn.k_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.31.self_attn.k_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.31.self_attn.v_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.31.self_attn.v_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.31.self_attn.o_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.31.self_attn.o_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.31.mlp.gate_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.31.mlp.gate_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.31.mlp.up_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.31.mlp.up_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.31.mlp.down_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.31.mlp.down_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.32.self_attn.q_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.32.self_attn.q_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.32.self_attn.k_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.32.self_attn.k_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.32.self_attn.v_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.32.self_attn.v_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.32.self_attn.o_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.32.self_attn.o_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.32.mlp.gate_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.32.mlp.gate_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.32.mlp.up_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.32.mlp.up_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.32.mlp.down_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.32.mlp.down_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.33.self_attn.q_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.33.self_attn.q_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.33.self_attn.k_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.33.self_attn.k_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.33.self_attn.v_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.33.self_attn.v_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.33.self_attn.o_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.33.self_attn.o_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.33.mlp.gate_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.33.mlp.gate_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.33.mlp.up_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.33.mlp.up_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.33.mlp.down_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.33.mlp.down_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.34.self_attn.q_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.34.self_attn.q_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.34.self_attn.k_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.34.self_attn.k_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.34.self_attn.v_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.34.self_attn.v_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.34.self_attn.o_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.34.self_attn.o_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.34.mlp.gate_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.34.mlp.gate_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.34.mlp.up_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.34.mlp.up_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.34.mlp.down_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.34.mlp.down_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.35.self_attn.q_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.35.self_attn.q_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.35.self_attn.k_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.35.self_attn.k_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.35.self_attn.v_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.35.self_attn.v_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.35.self_attn.o_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.35.self_attn.o_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.35.mlp.gate_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.35.mlp.gate_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.35.mlp.up_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.35.mlp.up_proj.lora_B.default.weight\n‚úÖ Enabled: base_model.model.model.layers.35.mlp.down_proj.lora_A.default.weight\n‚úÖ Enabled: base_model.model.model.layers.35.mlp.down_proj.lora_B.default.weight\nüìä Verification:\n   - Trainable parameter tensors: 504\n   - Trainable parameters: 14,966,784\n‚úÖ Ready for training!\n","output_type":"stream"}],"execution_count":63},{"id":"0c50f447-0c89-4272-bb6c-2c324c07f76f","cell_type":"code","source":"# COMPLETE LoRA RESET\nprint(\"üîß Setting up LoRA (FIXED VERSION)...\")\n\nfrom peft import get_peft_model, LoraConfig, TaskType\n\n# Remove any existing adapters\nif hasattr(model, 'peft_config'):\n    model = model.merge_and_unload()\n\n# Create fresh LoRA config\nlora_config = LoraConfig(\n    r=16,\n    lora_alpha=32,\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=TaskType.CAUSAL_LM,\n)\n\n# Apply LoRA\nmodel = get_peft_model(model, lora_config)\nmodel.print_trainable_parameters()\n\n# Force training mode\nmodel.train()\nmodel.enable_input_require_grads()\nfor param in model.parameters():\n    if param.requires_grad:\n        param.requires_grad = True\n\nprint(\"‚úÖ LoRA setup complete - ready for training!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-21T18:36:57.508962Z","iopub.execute_input":"2025-08-21T18:36:57.509670Z","iopub.status.idle":"2025-08-21T18:37:05.911015Z","shell.execute_reply.started":"2025-08-21T18:36:57.509644Z","shell.execute_reply":"2025-08-21T18:37:05.910266Z"}},"outputs":[{"name":"stdout","text":"üîß Setting up LoRA (FIXED VERSION)...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/peft/tuners/lora/bnb.py:351: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/peft/tuners/tuners_utils.py:167: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"trainable params: 29,933,568 || all params: 3,115,317,248 || trainable%: 0.9609\n‚úÖ LoRA setup complete - ready for training!\n","output_type":"stream"}],"execution_count":52},{"id":"bd306675","cell_type":"markdown","source":"## 6. Model Training Loop\n\nRunning the complete training pipeline with progress monitoring.","metadata":{}},{"id":"7bb7d4ab","cell_type":"code","source":"# Create trainer\nprint(\"üèãÔ∏è Creating trainer...\")\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    data_collator=data_collator,\n    tokenizer=tokenizer,\n)\n\nprint(\"‚úÖ Trainer created successfully\")\ncheck_gpu_memory()\nclear_memory()\nos.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-21T18:42:23.624508Z","iopub.execute_input":"2025-08-21T18:42:23.625305Z","iopub.status.idle":"2025-08-21T18:42:24.055660Z","shell.execute_reply.started":"2025-08-21T18:42:23.625275Z","shell.execute_reply":"2025-08-21T18:42:24.055101Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_201/361668662.py:4: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\nNo label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n","output_type":"stream"},{"name":"stdout","text":"üèãÔ∏è Creating trainer...\n‚úÖ Trainer created successfully\nüñ•Ô∏è  GPU Memory - Allocated: 4.02GB, Reserved: 8.45GB\n","output_type":"stream"}],"execution_count":64},{"id":"868e6384","cell_type":"code","source":"# Start training\nprint(\"\\nüî• Starting training...\")\nprint(\"=\" * 60)\nprint(f\"üìä Training Configuration Summary:\")\nprint(f\"   - Model: {config.model_name}\")\nprint(f\"   - Training samples: {len(train_dataset):,}\")\nprint(f\"   - Validation samples: {len(val_dataset):,}\")\nprint(f\"   - Epochs: {config.num_epochs}\")\nprint(f\"   - Batch size: {config.train_batch_size} (effective: {config.train_batch_size * config.gradient_accumulation_steps})\")\nprint(f\"   - Learning rate: {config.learning_rate}\")\nprint(f\"   - Max sequence length: {config.max_length}\")\nprint(f\"   - Steps per epoch: {len(train_dataset) // (config.train_batch_size * config.gradient_accumulation_steps)}\")\nprint(\"=\" * 60)\n\nstart_time = time.time()\n\ntry:\n    # Train the model\n    trainer.train()\n    \n    training_time = time.time() - start_time\n    print(f\"\\n‚úÖ Training completed successfully!\")\n    print(f\"‚è±Ô∏è  Total training time: {training_time / 3600:.2f} hours ({training_time / 60:.1f} minutes)\")\n    \nexcept Exception as e:\n    print(f\"‚ùå Training failed: {str(e)}\")\n    import traceback\n    traceback.print_exc()\n    \nfinally:\n    clear_memory()\n    check_gpu_memory()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-21T18:42:27.344938Z","iopub.execute_input":"2025-08-21T18:42:27.345578Z","iopub.status.idle":"2025-08-21T18:44:58.553107Z","shell.execute_reply.started":"2025-08-21T18:42:27.345550Z","shell.execute_reply":"2025-08-21T18:44:58.552086Z"}},"outputs":[{"name":"stdout","text":"\nüî• Starting training...\n============================================================\nüìä Training Configuration Summary:\n   - Model: Qwen/Qwen2.5-Coder-3B-Instruct\n   - Training samples: 14,049\n   - Validation samples: 1,756\n   - Epochs: 3\n   - Batch size: 1 (effective: 16)\n   - Learning rate: 0.0002\n   - Max sequence length: 512\n   - Steps per epoch: 878\n============================================================\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='7' max='2637' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [   7/2637 02:02 < 17:53:42, 0.04 it/s, Epoch 0.01/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"üñ•Ô∏è  GPU Memory - Allocated: 2.16GB, Reserved: 8.46GB\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_201/3688286965.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mtraining_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2238\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2239\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2240\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   2241\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2242\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2553\u001b[0m                     )\n\u001b[1;32m   2554\u001b[0m                     \u001b[0;32mwith\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2555\u001b[0;31m                         \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2556\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2557\u001b[0m                     if (\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   3789\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"scale_wrt_gas\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3790\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3791\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3792\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3793\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/accelerate/accelerator.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   2728\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2729\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscaler\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2730\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2731\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_lomo_optimizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2732\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlomo_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    624\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m             )\n\u001b[0;32m--> 626\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    627\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 823\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    824\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":65},{"id":"5a6dd2da","cell_type":"markdown","source":"## 7. Model Evaluation and Testing\n\nEvaluating model performance and generating sample unit tests.","metadata":{}},{"id":"8af8c9bd","cell_type":"code","source":"# Final evaluation on test set\nprint(\"üìä Running final evaluation on test set...\")\n\neval_results = trainer.evaluate(eval_dataset=test_dataset)\n\nprint(f\"\\nüìà Final Evaluation Results:\")\nprint(f\"   - Test Loss: {eval_results['eval_loss']:.4f}\")\nprint(f\"   - Test Perplexity: {np.exp(eval_results['eval_loss']):.2f}\")\n\n# Save evaluation results\nwith open(f\"{config.output_dir}/eval_results.json\", \"w\") as f:\n    json.dump(eval_results, f, indent=2)\n    \nprint(f\"‚úÖ Evaluation results saved to {config.output_dir}/eval_results.json\")","metadata":{},"outputs":[],"execution_count":null},{"id":"476fcf11","cell_type":"code","source":"# Test model with sample generations\ndef test_model_generation(model, tokenizer, test_data: List[Dict], num_samples: int = 5):\n    \"\"\"Test the trained model with sample unit test generations\"\"\"\n    print(f\"üß™ Testing model with {num_samples} sample generations...\\n\")\n    \n    model.eval()\n    \n    # Select random test samples\n    test_samples = np.random.choice(test_data, min(num_samples, len(test_data)), replace=False)\n    \n    for i, sample in enumerate(test_samples):\n        print(f\"{'='*80}\")\n        print(f\"Test Sample {i+1}/{num_samples}\")\n        print(f\"{'='*80}\")\n        \n        # Extract code from sample\n        if 'code' in sample:\n            code = sample['code']\n        else:\n            # Try to extract from tokenized data\n            input_ids = sample.get('input_ids', [])\n            if input_ids:\n                decoded = tokenizer.decode(input_ids[:200], skip_special_tokens=True)\n                # Extract code portion (simplified)\n                code = decoded.split('# Unit test:')[0].replace('# Generate a unit test for the following Python function:', '').strip()\n            else:\n                code = \"Sample code not available\"\n        \n        print(f\"üìù Original Code:\")\n        print(f\"{code[:300]}{'...' if len(code) > 300 else ''}\\n\")\n        \n        # Create generation prompt\n        prompt = f\"# Generate a unit test for the following Python function:\\n{code}\\n\\n# Unit test:\\n\"\n        \n        # Tokenize prompt\n        inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=512)\n        inputs = {k: v.to(model.device) for k, v in inputs.items()}\n        \n        # Generate unit test\n        with torch.no_grad():\n            outputs = model.generate(\n                **inputs,\n                max_new_tokens=300,\n                temperature=0.7,\n                do_sample=True,\n                top_p=0.9,\n                pad_token_id=tokenizer.eos_token_id,\n                eos_token_id=tokenizer.eos_token_id\n            )\n        \n        # Decode generated response\n        full_response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n        generated_test = full_response[len(prompt):].strip()\n        \n        print(f\"ü§ñ Generated Unit Test:\")\n        print(f\"{generated_test}\\n\")\n        \n        # Show original unit test if available\n        if 'unit_test' in sample:\n            print(f\"‚úÖ Original Unit Test:\")\n            print(f\"{sample['unit_test'][:300]}{'...' if len(sample['unit_test']) > 300 else ''}\\n\")\n        \n        print(\"\\n\")\n\n# Run sample generations\ntest_model_generation(model, tokenizer, test_data, num_samples=3)","metadata":{},"outputs":[],"execution_count":null},{"id":"0cd6bda1","cell_type":"markdown","source":"## 8. Save Trained Model\n\nSaving the fine-tuned model and creating downloadable archives.","metadata":{}},{"id":"63868a84","cell_type":"code","source":"# Save the trained model\nprint(\"üíæ Saving trained model...\")\n\n# Save model and tokenizer\ntrainer.save_model(config.output_dir)\ntokenizer.save_pretrained(config.output_dir)\n\nprint(f\"‚úÖ Model saved to: {config.output_dir}\")\n\n# Save training configuration\nconfig_dict = {\n    'model_name': config.model_name,\n    'max_length': config.max_length,\n    'train_batch_size': config.train_batch_size,\n    'gradient_accumulation_steps': config.gradient_accumulation_steps,\n    'num_epochs': config.num_epochs,\n    'learning_rate': config.learning_rate,\n    'lora_r': config.lora_r,\n    'lora_alpha': config.lora_alpha,\n    'lora_dropout': config.lora_dropout,\n    'target_modules': config.target_modules,\n    'training_samples': len(train_dataset),\n    'validation_samples': len(val_dataset),\n    'test_samples': len(test_dataset)\n}\n\nwith open(f\"{config.output_dir}/training_config.json\", \"w\") as f:\n    json.dump(config_dict, f, indent=2)\n\nprint(f\"‚úÖ Training configuration saved\")","metadata":{},"outputs":[],"execution_count":null},{"id":"b2e872c8","cell_type":"code","source":"# List saved files\nprint(\"üìÅ Saved model files:\")\nfor file in os.listdir(config.output_dir):\n    file_path = os.path.join(config.output_dir, file)\n    if os.path.isfile(file_path):\n        size_mb = os.path.getsize(file_path) / 1024 / 1024\n        print(f\"   - {file}: {size_mb:.1f} MB\")\n    else:\n        print(f\"   - {file}/ (directory)\")","metadata":{},"outputs":[],"execution_count":null},{"id":"7af559eb","cell_type":"code","source":"# Create downloadable archive\nprint(\"üì¶ Creating downloadable model archive...\")\n\narchive_name = \"qwen-coder-unittest-model\"\n!cd /kaggle/working && tar -czf {archive_name}.tar.gz qwen-coder-finetune/\n\n# Check archive size\narchive_path = f\"/kaggle/working/{archive_name}.tar.gz\"\nif os.path.exists(archive_path):\n    archive_size = os.path.getsize(archive_path) / 1024 / 1024\n    print(f\"‚úÖ Model archive created: {archive_name}.tar.gz ({archive_size:.1f} MB)\")\n    print(f\"üì• Download from: /kaggle/working/{archive_name}.tar.gz\")\nelse:\n    print(f\"‚ùå Failed to create archive\")\n\n# Final memory cleanup\nclear_memory()\nprint(\"\\nüéâ Training pipeline completed successfully!\")\nprint(f\"üìÅ Model saved in: {config.output_dir}\")\nprint(f\"üì¶ Download archive: {archive_name}.tar.gz\")","metadata":{},"outputs":[],"execution_count":null},{"id":"60ab514d","cell_type":"code","source":"# Training summary\nprint(\"\\n\" + \"=\"*80)\nprint(\"üéØ TRAINING SUMMARY\")\nprint(\"=\"*80)\nprint(f\"üìä Dataset: {len(train_data) + len(val_data) + len(test_data):,} total samples\")\nprint(f\"ü§ñ Model: {config.model_name}\")\nprint(f\"üîß Method: LoRA fine-tuning with 4-bit quantization\")\nprint(f\"‚ö° Hardware: Kaggle P100 GPU\")\nprint(f\"üìà Training: {config.num_epochs} epochs, {len(train_dataset):,} samples\")\nprint(f\"üíæ Output: {config.output_dir}\")\nprint(f\"üì¶ Archive: {archive_name}.tar.gz\")\nprint(\"\\n‚úÖ Ready for deployment and inference!\")\nprint(\"=\"*80)","metadata":{},"outputs":[],"execution_count":null}]}