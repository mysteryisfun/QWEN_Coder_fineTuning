{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":12829838,"sourceType":"datasetVersion","datasetId":8113918}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"1821982e","cell_type":"markdown","source":"# Qwen2.5-Coder Fine-tuning on Kaggle P100 GPU\n\nComplete end-to-end fine-tuning pipeline for Qwen2.5-Coder-3B to generate Python unit tests.\n\n**Dataset**: https://www.kaggle.com/datasets/ujwalsr/finetuning\n**GPU**: Kaggle P100 (16GB VRAM)\n**Method**: LoRA fine-tuning with 4-bit quantization\n\n---","metadata":{}},{"id":"225ad047","cell_type":"markdown","source":"## 1. Import Required Libraries\n\nInstalling and importing all necessary dependencies for training.","metadata":{}},{"id":"3223b341-b51b-47c9-8e72-fde33c7243f8","cell_type":"code","source":"# Install required packages\n!pip install -q transformers>=4.36.0\n!pip install -q peft>=0.7.0\n!pip install -q bitsandbytes>=0.41.0\n!pip install -q accelerate>=0.24.0\n!pip install -q datasets\n!pip install -q tqdm\n\nprint(\"✅ Packages installed successfully!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-21T18:10:20.842833Z","iopub.execute_input":"2025-08-21T18:10:20.843670Z","iopub.status.idle":"2025-08-21T18:11:47.985520Z","shell.execute_reply.started":"2025-08-21T18:10:20.843642Z","shell.execute_reply":"2025-08-21T18:11:47.984623Z"}},"outputs":[{"name":"stdout","text":"✅ Packages installed successfully!\n","output_type":"stream"}],"execution_count":3},{"id":"f8fd43e8","cell_type":"code","source":"# Import essential libraries\nimport os\nimport sys\nimport pickle\nimport torch\nimport torch.nn as nn\nimport gc\nimport time\nimport json\nfrom pathlib import Path\nfrom typing import Dict, List, Any, Optional\nfrom dataclasses import dataclass\nimport numpy as np\nimport pandas as pd\n\n# Transformers and PEFT\nfrom transformers import (\n    AutoTokenizer, AutoModelForCausalLM, \n    TrainingArguments, Trainer,\n    BitsAndBytesConfig, DataCollatorForLanguageModeling\n)\nfrom peft import LoraConfig, get_peft_model, TaskType\nfrom torch.utils.data import Dataset, DataLoader\nfrom tqdm import tqdm\n\n# Check GPU availability\nprint(f\"🖥️  CUDA available: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    print(f\"   GPU: {torch.cuda.get_device_name()}\")\n    print(f\"   GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f}GB\")\n    \n# Set random seeds for reproducibility\ntorch.manual_seed(42)\nnp.random.seed(42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-21T18:16:29.945587Z","iopub.execute_input":"2025-08-21T18:16:29.945867Z","iopub.status.idle":"2025-08-21T18:16:39.384992Z","shell.execute_reply.started":"2025-08-21T18:16:29.945844Z","shell.execute_reply":"2025-08-21T18:16:39.384336Z"}},"outputs":[{"name":"stderr","text":"2025-08-21 18:16:36.061967: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1755800196.084473     201 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1755800196.091188     201 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"🖥️  CUDA available: True\n   GPU: Tesla P100-PCIE-16GB\n   GPU Memory: 15.9GB\n","output_type":"stream"}],"execution_count":1},{"id":"2b2eb68b","cell_type":"markdown","source":"## 2. Load and Prepare Dataset\n\nLoading the three pickle files from the Kaggle dataset and examining the data structure.","metadata":{}},{"id":"f8481b33","cell_type":"code","source":"# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Dataset paths (Kaggle input directory)\ndataset_dir = \"/kaggle/input/finetuning\"\n\ntrain_pkl_path = f\"{dataset_dir}/train_split.pkl\"\nval_pkl_path = f\"{dataset_dir}/val_split.pkl\"\ntest_pkl_path = f\"{dataset_dir}/test_split.pkl\"\n\n# Verify files exist\nprint(f\"\\n🔍 Checking for dataset files in: {dataset_dir}\")\nfor path in [train_pkl_path, val_pkl_path, test_pkl_path]:\n    if os.path.exists(path):\n        file_size = os.path.getsize(path) / 1024 / 1024  # Size in MB\n        print(f\"✅ Found: {path} ({file_size:.1f} MB)\")\n    else:\n        print(f\"❌ Missing: {path}\")\n        \n# List all files in dataset directory\nif os.path.exists(dataset_dir):\n    print(f\"\\n📁 Dataset directory contents:\")\n    for file in os.listdir(dataset_dir):\n        file_path = os.path.join(dataset_dir, file)\n        if os.path.isfile(file_path):\n            file_size = os.path.getsize(file_path) / 1024 / 1024\n            print(f\"   - {file} ({file_size:.1f} MB)\")\n        else:\n            print(f\"   - {file}/ (directory)\")\nelse:\n    print(f\"❌ Dataset directory not found: {dataset_dir}\")\n    print(\"📋 Available input directories:\")\n    for item in os.listdir(\"/kaggle/input\"):\n        print(f\"   - /kaggle/input/{item}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-21T18:16:46.168582Z","iopub.execute_input":"2025-08-21T18:16:46.169257Z","iopub.status.idle":"2025-08-21T18:16:46.182125Z","shell.execute_reply.started":"2025-08-21T18:16:46.169225Z","shell.execute_reply":"2025-08-21T18:16:46.181358Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/finetuning/train_split.pkl\n/kaggle/input/finetuning/val_split.pkl\n/kaggle/input/finetuning/test_split.pkl\n\n🔍 Checking for dataset files in: /kaggle/input/finetuning\n✅ Found: /kaggle/input/finetuning/train_split.pkl (2198.0 MB)\n✅ Found: /kaggle/input/finetuning/val_split.pkl (274.8 MB)\n✅ Found: /kaggle/input/finetuning/test_split.pkl (273.7 MB)\n\n📁 Dataset directory contents:\n   - train_split.pkl (2198.0 MB)\n   - val_split.pkl (274.8 MB)\n   - test_split.pkl (273.7 MB)\n","output_type":"stream"}],"execution_count":2},{"id":"b2522788","cell_type":"code","source":"# Load pickle datasets\nprint(\"📂 Loading pickle datasets...\")\n\nwith open(train_pkl_path, 'rb') as f:\n    train_data = pickle.load(f)\n    \nwith open(val_pkl_path, 'rb') as f:\n    val_data = pickle.load(f)\n    \nwith open(test_pkl_path, 'rb') as f:\n    test_data = pickle.load(f)\n\nprint(f\"✅ Datasets loaded successfully:\")\nprint(f\"   - Training: {len(train_data):,} samples\")\nprint(f\"   - Validation: {len(val_data):,} samples\")\nprint(f\"   - Test: {len(test_data):,} samples\")\nprint(f\"   - Total: {len(train_data) + len(val_data) + len(test_data):,} samples\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-21T18:16:49.245991Z","iopub.execute_input":"2025-08-21T18:16:49.246381Z","iopub.status.idle":"2025-08-21T18:16:52.855420Z","shell.execute_reply.started":"2025-08-21T18:16:49.246345Z","shell.execute_reply":"2025-08-21T18:16:52.854690Z"}},"outputs":[{"name":"stdout","text":"📂 Loading pickle datasets...\n✅ Datasets loaded successfully:\n   - Training: 14,049 samples\n   - Validation: 1,756 samples\n   - Test: 1,757 samples\n   - Total: 17,562 samples\n","output_type":"stream"}],"execution_count":3},{"id":"5770d877","cell_type":"code","source":"# Examine data structure\nprint(\"🔍 Examining data structure...\")\n\nif len(train_data) > 0:\n    sample = train_data[0]\n    print(f\"\\n📋 Sample keys: {list(sample.keys())}\")\n    \n    # Check if data is pre-tokenized or raw text\n    if 'input_ids' in sample:\n        print(\"✅ Data is pre-tokenized\")\n        print(f\"   - Input IDs length: {len(sample['input_ids'])}\")\n        print(f\"   - Has attention mask: {'attention_mask' in sample}\")\n        print(f\"   - Has labels: {'labels' in sample}\")\n    else:\n        print(\"📝 Data contains raw text\")\n        for key in sample.keys():\n            if isinstance(sample[key], str):\n                print(f\"   - {key}: {len(sample[key])} characters\")\n                print(f\"     Preview: {sample[key][:100]}...\")\n            else:\n                print(f\"   - {key}: {type(sample[key])}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-21T18:16:55.465921Z","iopub.execute_input":"2025-08-21T18:16:55.466255Z","iopub.status.idle":"2025-08-21T18:16:55.472351Z","shell.execute_reply.started":"2025-08-21T18:16:55.466230Z","shell.execute_reply":"2025-08-21T18:16:55.471471Z"}},"outputs":[{"name":"stdout","text":"🔍 Examining data structure...\n\n📋 Sample keys: ['task_id', 'question', 'code_ground_truth', 'code_generate', 'unit_tests']\n📝 Data contains raw text\n   - task_id: <class 'int'>\n   - question: 1355 characters\n     Preview: As AtCoder Beginner Contest 100 is taking place, the office of AtCoder, Inc. is decorated with a seq...\n   - code_ground_truth: 147 characters\n     Preview: def max_operations_on_sequence(N, a):\n    ans = 0\n    for i in a:\n        while i % 2 == 0:\n        ...\n   - code_generate: 10335 characters\n     Preview: [{\"sol_id\": 0, \"code\": \"def max_operations_on_sequence(N, a):\\n    \\\"\\\"\\\"\\n    Calculate the maximum...\n   - unit_tests: 133288 characters\n     Preview: [{\"ut_id\": 0, \"code\": \"import unittest\\n\\nclass TestMaxOperationsOnSequence(unittest.TestCase):\\n\\n ...\n","output_type":"stream"}],"execution_count":4},{"id":"5ff68ef8","cell_type":"markdown","source":"## 3. Data Preprocessing and Feature Engineering\n\nSetting up the PyTorch Dataset class and data preprocessing pipeline.","metadata":{}},{"id":"814ed59f","cell_type":"code","source":"# Training configuration for Kaggle P100\n@dataclass\nclass KaggleTrainingConfig:\n    \"\"\"Optimized configuration for Kaggle P100 GPU (16GB VRAM)\"\"\"\n    \n    # Model configuration\n    model_name: str = \"Qwen/Qwen2.5-Coder-3B-Instruct\"\n    max_length: int = 512  # P100 can handle longer sequences\n    \n    # Training parameters - P100 optimized\n    train_batch_size: int = 1\n    eval_batch_size: int = 2\n    gradient_accumulation_steps: int = 16  # Effective batch size: 16\n    num_epochs: int = 3\n    learning_rate: float = 2e-4\n    weight_decay: float = 0.001\n    warmup_ratio: float = 0.03\n    \n    # LoRA configuration\n    lora_r: int = 8\n    lora_alpha: int = 16\n    lora_dropout: float = 0.05\n    target_modules: List[str] = None\n    \n    # Output configuration\n    output_dir: str = \"/kaggle/working/qwen-coder-finetune\"\n    run_name: str = \"qwen-coder-unittest-kaggle\"\n    logging_steps: int = 10\n    save_steps: int = 500\n    eval_steps: int = 500\n    \n    # Hardware optimization for P100\n    use_cuda: bool = True\n    mixed_precision: bool = True\n    gradient_checkpointing: bool = True  # Enable for P100\n    \n    def __post_init__(self):\n        if self.target_modules is None:\n            self.target_modules = [\n                \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n                \"gate_proj\", \"up_proj\", \"down_proj\"\n            ]\n\n# Initialize configuration\nconfig = KaggleTrainingConfig()\nprint(f\"📋 Training Configuration:\")\nprint(f\"   - Model: {config.model_name}\")\nprint(f\"   - Epochs: {config.num_epochs}\")\nprint(f\"   - Batch size: {config.train_batch_size}\")\nprint(f\"   - Effective batch size: {config.train_batch_size * config.gradient_accumulation_steps}\")\nprint(f\"   - Learning rate: {config.learning_rate}\")\nprint(f\"   - Max length: {config.max_length}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-21T18:41:32.805054Z","iopub.execute_input":"2025-08-21T18:41:32.805718Z","iopub.status.idle":"2025-08-21T18:41:32.815337Z","shell.execute_reply.started":"2025-08-21T18:41:32.805692Z","shell.execute_reply":"2025-08-21T18:41:32.814507Z"}},"outputs":[{"name":"stdout","text":"📋 Training Configuration:\n   - Model: Qwen/Qwen2.5-Coder-3B-Instruct\n   - Epochs: 3\n   - Batch size: 1\n   - Effective batch size: 16\n   - Learning rate: 0.0002\n   - Max length: 512\n","output_type":"stream"}],"execution_count":55},{"id":"c2391cdf","cell_type":"code","source":"# PyTorch Dataset class for unit test generation\nclass UnitTestDataset(Dataset):\n    \"\"\"PyTorch Dataset for unit test generation training\"\"\"\n    \n    def __init__(self, data: List[Dict], tokenizer, max_length: int = 1024):\n        self.data = data\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n        \n        print(f\"Dataset initialized with {len(data):,} samples\")\n        if len(data) > 0:\n            print(f\"Sample keys: {list(data[0].keys())}\")\n    \n    def __len__(self) -> int:\n        return len(self.data)\n    \n    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:\n        item = self.data[idx]\n        \n        # Handle pre-tokenized data\n        if 'input_ids' in item and 'labels' in item:\n            return {\n                'input_ids': torch.tensor(item['input_ids'][:self.max_length], dtype=torch.long),\n                'attention_mask': torch.tensor(item['attention_mask'][:self.max_length], dtype=torch.long),\n                'labels': torch.tensor(item['labels'][:self.max_length], dtype=torch.long)\n            }\n        \n        # Handle raw text data\n        else:\n            code = item.get('code', '')\n            unit_test = item.get('unit_test', '')\n            \n            # Create training prompt\n            prompt = f\"# Generate a unit test for the following Python function:\\n{code}\\n\\n# Unit test:\\n{unit_test}\"\n            \n            # Tokenize\n            encoded = self.tokenizer(\n                prompt,\n                truncation=True,\n                padding='max_length',\n                max_length=self.max_length,\n                return_tensors='pt'\n            )\n            \n            return {\n                'input_ids': encoded['input_ids'].squeeze(),\n                'attention_mask': encoded['attention_mask'].squeeze(),\n                'labels': encoded['input_ids'].squeeze().clone()\n            }\n\nprint(\"✅ UnitTestDataset class defined\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-21T18:41:38.724683Z","iopub.execute_input":"2025-08-21T18:41:38.725367Z","iopub.status.idle":"2025-08-21T18:41:38.733444Z","shell.execute_reply.started":"2025-08-21T18:41:38.725340Z","shell.execute_reply":"2025-08-21T18:41:38.732583Z"}},"outputs":[{"name":"stdout","text":"✅ UnitTestDataset class defined\n","output_type":"stream"}],"execution_count":56},{"id":"4fa73ebb","cell_type":"markdown","source":"## 4. Model Architecture Setup\n\nLoading the Qwen2.5-Coder model with quantization and setting up LoRA fine-tuning.","metadata":{}},{"id":"c058c5bd","cell_type":"code","source":"# Utility functions\ndef clear_memory():\n    \"\"\"Clear GPU and system memory\"\"\"\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n        torch.cuda.synchronize()\n    gc.collect()\n\ndef check_gpu_memory():\n    \"\"\"Check and print GPU memory usage\"\"\"\n    if torch.cuda.is_available():\n        allocated = torch.cuda.memory_allocated() / 1024**3\n        reserved = torch.cuda.memory_reserved() / 1024**3\n        print(f\"🖥️  GPU Memory - Allocated: {allocated:.2f}GB, Reserved: {reserved:.2f}GB\")\n    else:\n        print(\"❌ CUDA not available\")\n\ndef create_quantization_config() -> BitsAndBytesConfig:\n    \"\"\"Create 4-bit quantization configuration\"\"\"\n    return BitsAndBytesConfig(\n        load_in_4bit=True,\n        bnb_4bit_use_double_quant=True,\n        bnb_4bit_quant_type=\"nf4\",\n        bnb_4bit_compute_dtype=torch.bfloat16\n    )\n\nprint(\"✅ Utility functions defined\")\nclear_memory()\ncheck_gpu_memory()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-21T18:41:42.084681Z","iopub.execute_input":"2025-08-21T18:41:42.085254Z","iopub.status.idle":"2025-08-21T18:41:42.485657Z","shell.execute_reply.started":"2025-08-21T18:41:42.085230Z","shell.execute_reply":"2025-08-21T18:41:42.484881Z"}},"outputs":[{"name":"stdout","text":"✅ Utility functions defined\n🖥️  GPU Memory - Allocated: 2.04GB, Reserved: 8.45GB\n","output_type":"stream"}],"execution_count":57},{"id":"2458f78a","cell_type":"code","source":"# Load tokenizer\nprint(\"🤖 Loading tokenizer...\")\n\ntokenizer = AutoTokenizer.from_pretrained(\n    config.model_name,\n    trust_remote_code=True,\n    padding_side=\"left\"\n)\n\n# Add pad token if missing\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\n    print(\"✅ Added pad token\")\n\nprint(f\"✅ Tokenizer loaded: {config.model_name}\")\nprint(f\"   - Vocab size: {len(tokenizer)}\")\nprint(f\"   - Special tokens: pad={tokenizer.pad_token}, eos={tokenizer.eos_token}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-21T18:17:09.885516Z","iopub.execute_input":"2025-08-21T18:17:09.886298Z","iopub.status.idle":"2025-08-21T18:17:10.652933Z","shell.execute_reply.started":"2025-08-21T18:17:09.886269Z","shell.execute_reply":"2025-08-21T18:17:10.652080Z"}},"outputs":[{"name":"stdout","text":"🤖 Loading tokenizer...\n✅ Tokenizer loaded: Qwen/Qwen2.5-Coder-3B-Instruct\n   - Vocab size: 151665\n   - Special tokens: pad=<|endoftext|>, eos=<|im_end|>\n","output_type":"stream"}],"execution_count":8},{"id":"fbd0ff31-9582-44d7-a8e3-1e08ec8eeb0c","cell_type":"code","source":"# Fix bitsandbytes installation for Kaggle\n!pip install -U bitsandbytes>=0.41.0\n!pip install -U accelerate>=0.24.0\n\n# Restart kernel after installation\nimport os\nos._exit(00)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-21T18:16:09.924683Z","iopub.execute_input":"2025-08-21T18:16:09.924961Z","execution_failed":"2025-08-21T18:16:18.112Z"}},"outputs":[],"execution_count":null},{"id":"294df842","cell_type":"code","source":"# Load model with quantization\nprint(\"🤖 Loading model with quantization...\")\n\nquantization_config = create_quantization_config()\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    config.model_name,\n    quantization_config=quantization_config,\n    device_map=\"auto\",\n    trust_remote_code=True,\n    torch_dtype=torch.bfloat16,\n    attn_implementation=\"eager\"  # Avoid flash attention compatibility issues\n)\n\n# Resize token embeddings if needed\nif len(tokenizer) != model.config.vocab_size:\n    model.resize_token_embeddings(len(tokenizer))\n    print(f\"   - Resized token embeddings to {len(tokenizer)}\")\n\nprint(f\"✅ Model loaded: {config.model_name}\")\nprint(f\"   - Parameters: {model.num_parameters():,}\")\nprint(f\"   - Vocab size: {model.config.vocab_size}\")\ncheck_gpu_memory()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-21T18:41:48.744677Z","iopub.execute_input":"2025-08-21T18:41:48.745447Z","iopub.status.idle":"2025-08-21T18:41:54.373180Z","shell.execute_reply.started":"2025-08-21T18:41:48.745415Z","shell.execute_reply":"2025-08-21T18:41:54.372501Z"}},"outputs":[{"name":"stdout","text":"🤖 Loading model with quantization...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eb059cca82c04673b572d54dba82a113"}},"metadata":{}},{"name":"stdout","text":"   - Resized token embeddings to 151665\n✅ Model loaded: Qwen/Qwen2.5-Coder-3B-Instruct\n   - Parameters: 3,085,383,680\n   - Vocab size: 151665\n🖥️  GPU Memory - Allocated: 3.96GB, Reserved: 8.45GB\n","output_type":"stream"}],"execution_count":58},{"id":"bc0204fa","cell_type":"code","source":"# Setup LoRA fine-tuning\nprint(\"🔧 Setting up LoRA configuration...\")\n\nlora_config = LoraConfig(\n    r=config.lora_r,\n    lora_alpha=config.lora_alpha,\n    target_modules=config.target_modules,\n    lora_dropout=config.lora_dropout,\n    bias=\"none\",\n    task_type=TaskType.CAUSAL_LM,\n)\n\nmodel = get_peft_model(model, lora_config)\n\n# Print trainable parameters\ntrainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\ntotal_params = sum(p.numel() for p in model.parameters())\n\nprint(f\"✅ LoRA setup complete:\")\nprint(f\"   - Trainable params: {trainable_params:,}\")\nprint(f\"   - Total params: {total_params:,}\")\nprint(f\"   - Trainable percentage: {100 * trainable_params / total_params:.2f}%\")\n\ncheck_gpu_memory()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-21T18:41:55.904526Z","iopub.execute_input":"2025-08-21T18:41:55.905137Z","iopub.status.idle":"2025-08-21T18:41:56.323248Z","shell.execute_reply.started":"2025-08-21T18:41:55.905115Z","shell.execute_reply":"2025-08-21T18:41:56.322530Z"}},"outputs":[{"name":"stdout","text":"🔧 Setting up LoRA configuration...\n✅ LoRA setup complete:\n   - Trainable params: 14,966,784\n   - Total params: 1,713,084,416\n   - Trainable percentage: 0.87%\n🖥️  GPU Memory - Allocated: 4.02GB, Reserved: 8.45GB\n","output_type":"stream"}],"execution_count":59},{"id":"69837aef","cell_type":"markdown","source":"## 5. Training Configuration and Hyperparameters\n\nSetting up training arguments and creating PyTorch datasets.","metadata":{}},{"id":"d47688a7","cell_type":"code","source":"# Create PyTorch datasets\nprint(\"📊 Creating PyTorch datasets...\")\n\ntrain_dataset = UnitTestDataset(train_data, tokenizer, config.max_length)\nval_dataset = UnitTestDataset(val_data, tokenizer, config.max_length)\ntest_dataset = UnitTestDataset(test_data, tokenizer, config.max_length)\n\nprint(f\"✅ Datasets created:\")\nprint(f\"   - Training: {len(train_dataset):,} samples\")\nprint(f\"   - Validation: {len(val_dataset):,} samples\")\nprint(f\"   - Test: {len(test_dataset):,} samples\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-21T18:42:02.704619Z","iopub.execute_input":"2025-08-21T18:42:02.705189Z","iopub.status.idle":"2025-08-21T18:42:02.710274Z","shell.execute_reply.started":"2025-08-21T18:42:02.705154Z","shell.execute_reply":"2025-08-21T18:42:02.709430Z"}},"outputs":[{"name":"stdout","text":"📊 Creating PyTorch datasets...\nDataset initialized with 14,049 samples\nSample keys: ['task_id', 'question', 'code_ground_truth', 'code_generate', 'unit_tests']\nDataset initialized with 1,756 samples\nSample keys: ['task_id', 'question', 'code_ground_truth', 'code_generate', 'unit_tests']\nDataset initialized with 1,757 samples\nSample keys: ['task_id', 'question', 'code_ground_truth', 'code_generate', 'unit_tests']\n✅ Datasets created:\n   - Training: 14,049 samples\n   - Validation: 1,756 samples\n   - Test: 1,757 samples\n","output_type":"stream"}],"execution_count":60},{"id":"2ccfe6b6","cell_type":"code","source":"# Create data collator\ndata_collator = DataCollatorForLanguageModeling(\n    tokenizer=tokenizer,\n    mlm=False,\n    pad_to_multiple_of=8\n)\n\nprint(\"✅ Data collator created\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-21T18:42:06.104243Z","iopub.execute_input":"2025-08-21T18:42:06.104874Z","iopub.status.idle":"2025-08-21T18:42:06.109115Z","shell.execute_reply.started":"2025-08-21T18:42:06.104850Z","shell.execute_reply":"2025-08-21T18:42:06.108252Z"}},"outputs":[{"name":"stdout","text":"✅ Data collator created\n","output_type":"stream"}],"execution_count":61},{"id":"5d883b3d","cell_type":"code","source":"# Create training arguments\ntraining_args = TrainingArguments(\n    output_dir=config.output_dir,\n    run_name=config.run_name,\n    \n    # Training parameters\n    num_train_epochs=config.num_epochs,\n    per_device_train_batch_size=config.train_batch_size,\n    per_device_eval_batch_size=config.eval_batch_size,\n    gradient_accumulation_steps=config.gradient_accumulation_steps,\n    \n    # Optimization\n    learning_rate=config.learning_rate,\n    weight_decay=config.weight_decay,\n    warmup_ratio=config.warmup_ratio,\n    \n    # Hardware optimization for P100\n    fp16=True,\n    bf16=False,\n    gradient_checkpointing=False,\n    dataloader_pin_memory=False,\n    \n    # Logging and saving\n    logging_steps=config.logging_steps,\n    eval_steps=config.eval_steps,\n    save_steps=config.save_steps,\n    eval_strategy=\"steps\",\n    save_strategy=\"steps\",\n    \n    # Model selection\n    remove_unused_columns=False,\n    load_best_model_at_end=True,\n    metric_for_best_model=\"eval_loss\",\n    greater_is_better=False,\n    \n    # Disable external logging\n    report_to=[]\n)\n\nprint(\"✅ Training arguments created\")\nprint(f\"   - Total training steps: {len(train_dataset) // (config.train_batch_size * config.gradient_accumulation_steps) * config.num_epochs}\")\nprint(f\"   - Steps per epoch: {len(train_dataset) // (config.train_batch_size * config.gradient_accumulation_steps)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-21T18:42:08.264924Z","iopub.execute_input":"2025-08-21T18:42:08.265512Z","iopub.status.idle":"2025-08-21T18:42:08.294988Z","shell.execute_reply.started":"2025-08-21T18:42:08.265487Z","shell.execute_reply":"2025-08-21T18:42:08.294360Z"}},"outputs":[{"name":"stdout","text":"✅ Training arguments created\n   - Total training steps: 2634\n   - Steps per epoch: 878\n","output_type":"stream"}],"execution_count":62},{"id":"ad8b6875-6f3f-439a-82a1-f4e2eafe1aaa","cell_type":"code","source":"# CRITICAL FIX: Enable gradients for LoRA parameters\nprint(\"🔧 Fixing LoRA parameter gradients...\")\n\n# Method 1: Enable gradients for all LoRA parameters\nfor name, param in model.named_parameters():\n    if any(keyword in name for keyword in ['lora_A', 'lora_B', 'lora_embedding']):\n        param.requires_grad = True\n        print(f\"✅ Enabled: {name}\")\n\n# Method 2: Alternative - enable all trainable parameters\nmodel.train()\nfor param in model.parameters():\n    if param.requires_grad:\n        param.requires_grad = True\n\n# Verify fix\ntrainable_count = sum(1 for p in model.parameters() if p.requires_grad)\ntotal_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n\nprint(f\"📊 Verification:\")\nprint(f\"   - Trainable parameter tensors: {trainable_count}\")\nprint(f\"   - Trainable parameters: {total_params:,}\")\n\nif trainable_count == 0:\n    print(\"❌ STILL NO TRAINABLE PARAMETERS!\")\n    # Force enable LoRA\n    model.enable_adapters()\n    print(\"🔄 Forced LoRA adapter activation\")\nelse:\n    print(\"✅ Ready for training!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-21T18:42:15.145495Z","iopub.execute_input":"2025-08-21T18:42:15.146267Z","iopub.status.idle":"2025-08-21T18:42:15.194358Z","shell.execute_reply.started":"2025-08-21T18:42:15.146245Z","shell.execute_reply":"2025-08-21T18:42:15.193540Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"🔧 Fixing LoRA parameter gradients...\n✅ Enabled: base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.0.mlp.gate_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.0.mlp.gate_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.0.mlp.up_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.0.mlp.up_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.0.mlp.down_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.0.mlp.down_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.1.self_attn.k_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.1.self_attn.k_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.1.self_attn.o_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.1.self_attn.o_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.1.mlp.gate_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.1.mlp.gate_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.1.mlp.up_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.1.mlp.up_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.1.mlp.down_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.1.mlp.down_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.2.self_attn.k_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.2.self_attn.k_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.2.self_attn.o_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.2.self_attn.o_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.2.mlp.gate_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.2.mlp.gate_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.2.mlp.up_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.2.mlp.up_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.2.mlp.down_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.2.mlp.down_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.3.self_attn.k_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.3.self_attn.k_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.3.self_attn.o_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.3.self_attn.o_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.3.mlp.gate_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.3.mlp.gate_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.3.mlp.up_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.3.mlp.up_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.3.mlp.down_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.3.mlp.down_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.4.self_attn.k_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.4.self_attn.k_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.4.self_attn.o_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.4.self_attn.o_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.4.mlp.gate_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.4.mlp.gate_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.4.mlp.up_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.4.mlp.up_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.4.mlp.down_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.4.mlp.down_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.5.self_attn.k_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.5.self_attn.k_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.5.self_attn.o_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.5.self_attn.o_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.5.mlp.gate_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.5.mlp.gate_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.5.mlp.up_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.5.mlp.up_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.5.mlp.down_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.5.mlp.down_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.6.self_attn.k_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.6.self_attn.k_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.6.self_attn.o_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.6.self_attn.o_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.6.mlp.gate_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.6.mlp.gate_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.6.mlp.up_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.6.mlp.up_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.6.mlp.down_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.6.mlp.down_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.7.self_attn.k_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.7.self_attn.k_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.7.self_attn.o_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.7.self_attn.o_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.7.mlp.gate_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.7.mlp.gate_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.7.mlp.up_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.7.mlp.up_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.7.mlp.down_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.7.mlp.down_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.8.self_attn.k_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.8.self_attn.k_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.8.self_attn.o_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.8.self_attn.o_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.8.mlp.gate_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.8.mlp.gate_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.8.mlp.up_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.8.mlp.up_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.8.mlp.down_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.8.mlp.down_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.9.self_attn.k_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.9.self_attn.k_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.9.self_attn.o_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.9.self_attn.o_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.9.mlp.gate_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.9.mlp.gate_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.9.mlp.up_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.9.mlp.up_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.9.mlp.down_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.9.mlp.down_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.10.self_attn.k_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.10.self_attn.k_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.10.self_attn.o_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.10.self_attn.o_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.10.mlp.gate_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.10.mlp.gate_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.10.mlp.up_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.10.mlp.up_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.10.mlp.down_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.10.mlp.down_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.11.self_attn.k_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.11.self_attn.k_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.11.self_attn.o_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.11.self_attn.o_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.11.mlp.gate_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.11.mlp.gate_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.11.mlp.up_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.11.mlp.up_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.11.mlp.down_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.11.mlp.down_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.12.self_attn.k_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.12.self_attn.k_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.12.self_attn.o_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.12.self_attn.o_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.12.mlp.gate_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.12.mlp.gate_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.12.mlp.up_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.12.mlp.up_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.12.mlp.down_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.12.mlp.down_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.13.self_attn.k_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.13.self_attn.k_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.13.self_attn.o_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.13.self_attn.o_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.13.mlp.gate_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.13.mlp.gate_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.13.mlp.up_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.13.mlp.up_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.13.mlp.down_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.13.mlp.down_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.14.self_attn.k_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.14.self_attn.k_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.14.self_attn.o_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.14.self_attn.o_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.14.mlp.gate_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.14.mlp.gate_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.14.mlp.up_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.14.mlp.up_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.14.mlp.down_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.14.mlp.down_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.15.self_attn.k_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.15.self_attn.k_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.15.self_attn.o_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.15.self_attn.o_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.15.mlp.gate_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.15.mlp.gate_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.15.mlp.up_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.15.mlp.up_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.15.mlp.down_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.15.mlp.down_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.16.self_attn.k_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.16.self_attn.k_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.16.self_attn.o_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.16.self_attn.o_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.16.mlp.gate_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.16.mlp.gate_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.16.mlp.up_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.16.mlp.up_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.16.mlp.down_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.16.mlp.down_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.17.self_attn.k_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.17.self_attn.k_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.17.self_attn.o_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.17.self_attn.o_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.17.mlp.gate_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.17.mlp.gate_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.17.mlp.up_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.17.mlp.up_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.17.mlp.down_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.17.mlp.down_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.18.self_attn.k_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.18.self_attn.k_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.18.self_attn.o_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.18.self_attn.o_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.18.mlp.gate_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.18.mlp.gate_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.18.mlp.up_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.18.mlp.up_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.18.mlp.down_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.18.mlp.down_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.19.self_attn.k_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.19.self_attn.k_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.19.self_attn.o_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.19.self_attn.o_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.19.mlp.gate_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.19.mlp.gate_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.19.mlp.up_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.19.mlp.up_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.19.mlp.down_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.19.mlp.down_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.20.self_attn.k_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.20.self_attn.k_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.20.self_attn.o_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.20.self_attn.o_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.20.mlp.gate_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.20.mlp.gate_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.20.mlp.up_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.20.mlp.up_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.20.mlp.down_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.20.mlp.down_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.21.self_attn.k_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.21.self_attn.k_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.21.self_attn.o_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.21.self_attn.o_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.21.mlp.gate_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.21.mlp.gate_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.21.mlp.up_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.21.mlp.up_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.21.mlp.down_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.21.mlp.down_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.22.self_attn.k_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.22.self_attn.k_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.22.self_attn.o_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.22.self_attn.o_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.22.mlp.gate_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.22.mlp.gate_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.22.mlp.up_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.22.mlp.up_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.22.mlp.down_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.22.mlp.down_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.23.self_attn.k_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.23.self_attn.k_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.23.self_attn.o_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.23.self_attn.o_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.23.mlp.gate_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.23.mlp.gate_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.23.mlp.up_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.23.mlp.up_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.23.mlp.down_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.23.mlp.down_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.24.self_attn.q_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.24.self_attn.q_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.24.self_attn.k_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.24.self_attn.k_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.24.self_attn.v_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.24.self_attn.v_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.24.self_attn.o_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.24.self_attn.o_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.24.mlp.gate_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.24.mlp.gate_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.24.mlp.up_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.24.mlp.up_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.24.mlp.down_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.24.mlp.down_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.25.self_attn.q_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.25.self_attn.q_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.25.self_attn.k_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.25.self_attn.k_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.25.self_attn.v_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.25.self_attn.v_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.25.self_attn.o_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.25.self_attn.o_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.25.mlp.gate_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.25.mlp.gate_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.25.mlp.up_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.25.mlp.up_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.25.mlp.down_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.25.mlp.down_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.26.self_attn.q_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.26.self_attn.q_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.26.self_attn.k_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.26.self_attn.k_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.26.self_attn.v_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.26.self_attn.v_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.26.self_attn.o_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.26.self_attn.o_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.26.mlp.gate_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.26.mlp.gate_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.26.mlp.up_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.26.mlp.up_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.26.mlp.down_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.26.mlp.down_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.27.self_attn.q_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.27.self_attn.q_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.27.self_attn.k_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.27.self_attn.k_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.27.self_attn.v_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.27.self_attn.v_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.27.self_attn.o_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.27.self_attn.o_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.27.mlp.gate_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.27.mlp.gate_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.27.mlp.up_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.27.mlp.up_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.27.mlp.down_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.27.mlp.down_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.28.self_attn.q_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.28.self_attn.q_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.28.self_attn.k_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.28.self_attn.k_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.28.self_attn.v_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.28.self_attn.v_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.28.self_attn.o_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.28.self_attn.o_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.28.mlp.gate_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.28.mlp.gate_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.28.mlp.up_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.28.mlp.up_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.28.mlp.down_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.28.mlp.down_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.29.self_attn.q_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.29.self_attn.q_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.29.self_attn.k_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.29.self_attn.k_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.29.self_attn.v_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.29.self_attn.v_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.29.self_attn.o_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.29.self_attn.o_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.29.mlp.gate_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.29.mlp.gate_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.29.mlp.up_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.29.mlp.up_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.29.mlp.down_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.29.mlp.down_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.30.self_attn.q_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.30.self_attn.q_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.30.self_attn.k_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.30.self_attn.k_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.30.self_attn.v_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.30.self_attn.v_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.30.self_attn.o_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.30.self_attn.o_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.30.mlp.gate_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.30.mlp.gate_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.30.mlp.up_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.30.mlp.up_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.30.mlp.down_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.30.mlp.down_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.31.self_attn.q_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.31.self_attn.q_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.31.self_attn.k_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.31.self_attn.k_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.31.self_attn.v_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.31.self_attn.v_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.31.self_attn.o_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.31.self_attn.o_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.31.mlp.gate_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.31.mlp.gate_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.31.mlp.up_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.31.mlp.up_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.31.mlp.down_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.31.mlp.down_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.32.self_attn.q_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.32.self_attn.q_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.32.self_attn.k_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.32.self_attn.k_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.32.self_attn.v_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.32.self_attn.v_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.32.self_attn.o_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.32.self_attn.o_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.32.mlp.gate_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.32.mlp.gate_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.32.mlp.up_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.32.mlp.up_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.32.mlp.down_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.32.mlp.down_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.33.self_attn.q_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.33.self_attn.q_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.33.self_attn.k_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.33.self_attn.k_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.33.self_attn.v_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.33.self_attn.v_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.33.self_attn.o_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.33.self_attn.o_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.33.mlp.gate_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.33.mlp.gate_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.33.mlp.up_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.33.mlp.up_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.33.mlp.down_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.33.mlp.down_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.34.self_attn.q_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.34.self_attn.q_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.34.self_attn.k_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.34.self_attn.k_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.34.self_attn.v_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.34.self_attn.v_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.34.self_attn.o_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.34.self_attn.o_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.34.mlp.gate_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.34.mlp.gate_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.34.mlp.up_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.34.mlp.up_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.34.mlp.down_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.34.mlp.down_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.35.self_attn.q_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.35.self_attn.q_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.35.self_attn.k_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.35.self_attn.k_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.35.self_attn.v_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.35.self_attn.v_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.35.self_attn.o_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.35.self_attn.o_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.35.mlp.gate_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.35.mlp.gate_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.35.mlp.up_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.35.mlp.up_proj.lora_B.default.weight\n✅ Enabled: base_model.model.model.layers.35.mlp.down_proj.lora_A.default.weight\n✅ Enabled: base_model.model.model.layers.35.mlp.down_proj.lora_B.default.weight\n📊 Verification:\n   - Trainable parameter tensors: 504\n   - Trainable parameters: 14,966,784\n✅ Ready for training!\n","output_type":"stream"}],"execution_count":63},{"id":"0c50f447-0c89-4272-bb6c-2c324c07f76f","cell_type":"code","source":"# COMPLETE LoRA RESET\nprint(\"🔧 Setting up LoRA (FIXED VERSION)...\")\n\nfrom peft import get_peft_model, LoraConfig, TaskType\n\n# Remove any existing adapters\nif hasattr(model, 'peft_config'):\n    model = model.merge_and_unload()\n\n# Create fresh LoRA config\nlora_config = LoraConfig(\n    r=16,\n    lora_alpha=32,\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=TaskType.CAUSAL_LM,\n)\n\n# Apply LoRA\nmodel = get_peft_model(model, lora_config)\nmodel.print_trainable_parameters()\n\n# Force training mode\nmodel.train()\nmodel.enable_input_require_grads()\nfor param in model.parameters():\n    if param.requires_grad:\n        param.requires_grad = True\n\nprint(\"✅ LoRA setup complete - ready for training!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-21T18:36:57.508962Z","iopub.execute_input":"2025-08-21T18:36:57.509670Z","iopub.status.idle":"2025-08-21T18:37:05.911015Z","shell.execute_reply.started":"2025-08-21T18:36:57.509644Z","shell.execute_reply":"2025-08-21T18:37:05.910266Z"}},"outputs":[{"name":"stdout","text":"🔧 Setting up LoRA (FIXED VERSION)...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/peft/tuners/lora/bnb.py:351: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/peft/tuners/tuners_utils.py:167: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"trainable params: 29,933,568 || all params: 3,115,317,248 || trainable%: 0.9609\n✅ LoRA setup complete - ready for training!\n","output_type":"stream"}],"execution_count":52},{"id":"bd306675","cell_type":"markdown","source":"## 6. Model Training Loop\n\nRunning the complete training pipeline with progress monitoring.","metadata":{}},{"id":"7bb7d4ab","cell_type":"code","source":"# Create trainer\nprint(\"🏋️ Creating trainer...\")\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    data_collator=data_collator,\n    tokenizer=tokenizer,\n)\n\nprint(\"✅ Trainer created successfully\")\ncheck_gpu_memory()\nclear_memory()\nos.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-21T18:42:23.624508Z","iopub.execute_input":"2025-08-21T18:42:23.625305Z","iopub.status.idle":"2025-08-21T18:42:24.055660Z","shell.execute_reply.started":"2025-08-21T18:42:23.625275Z","shell.execute_reply":"2025-08-21T18:42:24.055101Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_201/361668662.py:4: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\nNo label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n","output_type":"stream"},{"name":"stdout","text":"🏋️ Creating trainer...\n✅ Trainer created successfully\n🖥️  GPU Memory - Allocated: 4.02GB, Reserved: 8.45GB\n","output_type":"stream"}],"execution_count":64},{"id":"868e6384","cell_type":"code","source":"# Start training\nprint(\"\\n🔥 Starting training...\")\nprint(\"=\" * 60)\nprint(f\"📊 Training Configuration Summary:\")\nprint(f\"   - Model: {config.model_name}\")\nprint(f\"   - Training samples: {len(train_dataset):,}\")\nprint(f\"   - Validation samples: {len(val_dataset):,}\")\nprint(f\"   - Epochs: {config.num_epochs}\")\nprint(f\"   - Batch size: {config.train_batch_size} (effective: {config.train_batch_size * config.gradient_accumulation_steps})\")\nprint(f\"   - Learning rate: {config.learning_rate}\")\nprint(f\"   - Max sequence length: {config.max_length}\")\nprint(f\"   - Steps per epoch: {len(train_dataset) // (config.train_batch_size * config.gradient_accumulation_steps)}\")\nprint(\"=\" * 60)\n\nstart_time = time.time()\n\ntry:\n    # Train the model\n    trainer.train()\n    \n    training_time = time.time() - start_time\n    print(f\"\\n✅ Training completed successfully!\")\n    print(f\"⏱️  Total training time: {training_time / 3600:.2f} hours ({training_time / 60:.1f} minutes)\")\n    \nexcept Exception as e:\n    print(f\"❌ Training failed: {str(e)}\")\n    import traceback\n    traceback.print_exc()\n    \nfinally:\n    clear_memory()\n    check_gpu_memory()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-21T18:42:27.344938Z","iopub.execute_input":"2025-08-21T18:42:27.345578Z","iopub.status.idle":"2025-08-21T18:44:58.553107Z","shell.execute_reply.started":"2025-08-21T18:42:27.345550Z","shell.execute_reply":"2025-08-21T18:44:58.552086Z"}},"outputs":[{"name":"stdout","text":"\n🔥 Starting training...\n============================================================\n📊 Training Configuration Summary:\n   - Model: Qwen/Qwen2.5-Coder-3B-Instruct\n   - Training samples: 14,049\n   - Validation samples: 1,756\n   - Epochs: 3\n   - Batch size: 1 (effective: 16)\n   - Learning rate: 0.0002\n   - Max sequence length: 512\n   - Steps per epoch: 878\n============================================================\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='7' max='2637' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [   7/2637 02:02 < 17:53:42, 0.04 it/s, Epoch 0.01/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"🖥️  GPU Memory - Allocated: 2.16GB, Reserved: 8.46GB\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_201/3688286965.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mtraining_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2238\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2239\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2240\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   2241\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2242\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2553\u001b[0m                     )\n\u001b[1;32m   2554\u001b[0m                     \u001b[0;32mwith\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2555\u001b[0;31m                         \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2556\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2557\u001b[0m                     if (\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   3789\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"scale_wrt_gas\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3790\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3791\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3792\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3793\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/accelerate/accelerator.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   2728\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2729\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscaler\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2730\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2731\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_lomo_optimizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2732\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlomo_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    624\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m             )\n\u001b[0;32m--> 626\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    627\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 823\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    824\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":65},{"id":"5a6dd2da","cell_type":"markdown","source":"## 7. Model Evaluation and Testing\n\nEvaluating model performance and generating sample unit tests.","metadata":{}},{"id":"8af8c9bd","cell_type":"code","source":"# Final evaluation on test set\nprint(\"📊 Running final evaluation on test set...\")\n\neval_results = trainer.evaluate(eval_dataset=test_dataset)\n\nprint(f\"\\n📈 Final Evaluation Results:\")\nprint(f\"   - Test Loss: {eval_results['eval_loss']:.4f}\")\nprint(f\"   - Test Perplexity: {np.exp(eval_results['eval_loss']):.2f}\")\n\n# Save evaluation results\nwith open(f\"{config.output_dir}/eval_results.json\", \"w\") as f:\n    json.dump(eval_results, f, indent=2)\n    \nprint(f\"✅ Evaluation results saved to {config.output_dir}/eval_results.json\")","metadata":{},"outputs":[],"execution_count":null},{"id":"476fcf11","cell_type":"code","source":"# Test model with sample generations\ndef test_model_generation(model, tokenizer, test_data: List[Dict], num_samples: int = 5):\n    \"\"\"Test the trained model with sample unit test generations\"\"\"\n    print(f\"🧪 Testing model with {num_samples} sample generations...\\n\")\n    \n    model.eval()\n    \n    # Select random test samples\n    test_samples = np.random.choice(test_data, min(num_samples, len(test_data)), replace=False)\n    \n    for i, sample in enumerate(test_samples):\n        print(f\"{'='*80}\")\n        print(f\"Test Sample {i+1}/{num_samples}\")\n        print(f\"{'='*80}\")\n        \n        # Extract code from sample\n        if 'code' in sample:\n            code = sample['code']\n        else:\n            # Try to extract from tokenized data\n            input_ids = sample.get('input_ids', [])\n            if input_ids:\n                decoded = tokenizer.decode(input_ids[:200], skip_special_tokens=True)\n                # Extract code portion (simplified)\n                code = decoded.split('# Unit test:')[0].replace('# Generate a unit test for the following Python function:', '').strip()\n            else:\n                code = \"Sample code not available\"\n        \n        print(f\"📝 Original Code:\")\n        print(f\"{code[:300]}{'...' if len(code) > 300 else ''}\\n\")\n        \n        # Create generation prompt\n        prompt = f\"# Generate a unit test for the following Python function:\\n{code}\\n\\n# Unit test:\\n\"\n        \n        # Tokenize prompt\n        inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=512)\n        inputs = {k: v.to(model.device) for k, v in inputs.items()}\n        \n        # Generate unit test\n        with torch.no_grad():\n            outputs = model.generate(\n                **inputs,\n                max_new_tokens=300,\n                temperature=0.7,\n                do_sample=True,\n                top_p=0.9,\n                pad_token_id=tokenizer.eos_token_id,\n                eos_token_id=tokenizer.eos_token_id\n            )\n        \n        # Decode generated response\n        full_response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n        generated_test = full_response[len(prompt):].strip()\n        \n        print(f\"🤖 Generated Unit Test:\")\n        print(f\"{generated_test}\\n\")\n        \n        # Show original unit test if available\n        if 'unit_test' in sample:\n            print(f\"✅ Original Unit Test:\")\n            print(f\"{sample['unit_test'][:300]}{'...' if len(sample['unit_test']) > 300 else ''}\\n\")\n        \n        print(\"\\n\")\n\n# Run sample generations\ntest_model_generation(model, tokenizer, test_data, num_samples=3)","metadata":{},"outputs":[],"execution_count":null},{"id":"0cd6bda1","cell_type":"markdown","source":"## 8. Save Trained Model\n\nSaving the fine-tuned model and creating downloadable archives.","metadata":{}},{"id":"63868a84","cell_type":"code","source":"# Save the trained model\nprint(\"💾 Saving trained model...\")\n\n# Save model and tokenizer\ntrainer.save_model(config.output_dir)\ntokenizer.save_pretrained(config.output_dir)\n\nprint(f\"✅ Model saved to: {config.output_dir}\")\n\n# Save training configuration\nconfig_dict = {\n    'model_name': config.model_name,\n    'max_length': config.max_length,\n    'train_batch_size': config.train_batch_size,\n    'gradient_accumulation_steps': config.gradient_accumulation_steps,\n    'num_epochs': config.num_epochs,\n    'learning_rate': config.learning_rate,\n    'lora_r': config.lora_r,\n    'lora_alpha': config.lora_alpha,\n    'lora_dropout': config.lora_dropout,\n    'target_modules': config.target_modules,\n    'training_samples': len(train_dataset),\n    'validation_samples': len(val_dataset),\n    'test_samples': len(test_dataset)\n}\n\nwith open(f\"{config.output_dir}/training_config.json\", \"w\") as f:\n    json.dump(config_dict, f, indent=2)\n\nprint(f\"✅ Training configuration saved\")","metadata":{},"outputs":[],"execution_count":null},{"id":"b2e872c8","cell_type":"code","source":"# List saved files\nprint(\"📁 Saved model files:\")\nfor file in os.listdir(config.output_dir):\n    file_path = os.path.join(config.output_dir, file)\n    if os.path.isfile(file_path):\n        size_mb = os.path.getsize(file_path) / 1024 / 1024\n        print(f\"   - {file}: {size_mb:.1f} MB\")\n    else:\n        print(f\"   - {file}/ (directory)\")","metadata":{},"outputs":[],"execution_count":null},{"id":"7af559eb","cell_type":"code","source":"# Create downloadable archive\nprint(\"📦 Creating downloadable model archive...\")\n\narchive_name = \"qwen-coder-unittest-model\"\n!cd /kaggle/working && tar -czf {archive_name}.tar.gz qwen-coder-finetune/\n\n# Check archive size\narchive_path = f\"/kaggle/working/{archive_name}.tar.gz\"\nif os.path.exists(archive_path):\n    archive_size = os.path.getsize(archive_path) / 1024 / 1024\n    print(f\"✅ Model archive created: {archive_name}.tar.gz ({archive_size:.1f} MB)\")\n    print(f\"📥 Download from: /kaggle/working/{archive_name}.tar.gz\")\nelse:\n    print(f\"❌ Failed to create archive\")\n\n# Final memory cleanup\nclear_memory()\nprint(\"\\n🎉 Training pipeline completed successfully!\")\nprint(f\"📁 Model saved in: {config.output_dir}\")\nprint(f\"📦 Download archive: {archive_name}.tar.gz\")","metadata":{},"outputs":[],"execution_count":null},{"id":"60ab514d","cell_type":"code","source":"# Training summary\nprint(\"\\n\" + \"=\"*80)\nprint(\"🎯 TRAINING SUMMARY\")\nprint(\"=\"*80)\nprint(f\"📊 Dataset: {len(train_data) + len(val_data) + len(test_data):,} total samples\")\nprint(f\"🤖 Model: {config.model_name}\")\nprint(f\"🔧 Method: LoRA fine-tuning with 4-bit quantization\")\nprint(f\"⚡ Hardware: Kaggle P100 GPU\")\nprint(f\"📈 Training: {config.num_epochs} epochs, {len(train_dataset):,} samples\")\nprint(f\"💾 Output: {config.output_dir}\")\nprint(f\"📦 Archive: {archive_name}.tar.gz\")\nprint(\"\\n✅ Ready for deployment and inference!\")\nprint(\"=\"*80)","metadata":{},"outputs":[],"execution_count":null}]}