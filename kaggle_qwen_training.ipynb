{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1821982e",
   "metadata": {},
   "source": [
    "# Qwen2.5-Coder Fine-tuning on Kaggle P100 GPU\n",
    "\n",
    "Complete end-to-end fine-tuning pipeline for Qwen2.5-Coder-3B to generate Python unit tests.\n",
    "\n",
    "**Dataset**: https://www.kaggle.com/datasets/ujwalsr/finetuning\n",
    "**GPU**: Kaggle P100 (16GB VRAM)\n",
    "**Method**: LoRA fine-tuning with 4-bit quantization\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225ad047",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries\n",
    "\n",
    "Installing and importing all necessary dependencies for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb79690",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q transformers>=4.36.0\n",
    "!pip install -q peft>=0.7.0\n",
    "!pip install -q bitsandbytes>=0.41.0\n",
    "!pip install -q accelerate>=0.24.0\n",
    "!pip install -q datasets\n",
    "!pip install -q tqdm\n",
    "\n",
    "print(\"‚úÖ Packages installed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8fd43e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import essential libraries\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import gc\n",
    "import time\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Any, Optional\n",
    "from dataclasses import dataclass\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Transformers and PEFT\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForCausalLM, \n",
    "    TrainingArguments, Trainer,\n",
    "    BitsAndBytesConfig, DataCollatorForLanguageModeling\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Check GPU availability\n",
    "print(f\"üñ•Ô∏è  CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"   GPU: {torch.cuda.get_device_name()}\")\n",
    "    print(f\"   GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f}GB\")\n",
    "    \n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b2eb68b",
   "metadata": {},
   "source": [
    "## 2. Load and Prepare Dataset\n",
    "\n",
    "Loading the three pickle files from the Kaggle dataset and examining the data structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8481b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# Dataset paths (Kaggle input directory)\n",
    "dataset_dir = \"/kaggle/input/finetuning\"\n",
    "\n",
    "train_pkl_path = f\"{dataset_dir}/train_dataset.pkl\"\n",
    "val_pkl_path = f\"{dataset_dir}/val_dataset.pkl\"\n",
    "test_pkl_path = f\"{dataset_dir}/test_dataset.pkl\"\n",
    "\n",
    "# Verify files exist\n",
    "print(f\"\\nüîç Checking for dataset files in: {dataset_dir}\")\n",
    "for path in [train_pkl_path, val_pkl_path, test_pkl_path]:\n",
    "    if os.path.exists(path):\n",
    "        file_size = os.path.getsize(path) / 1024 / 1024  # Size in MB\n",
    "        print(f\"‚úÖ Found: {path} ({file_size:.1f} MB)\")\n",
    "    else:\n",
    "        print(f\"‚ùå Missing: {path}\")\n",
    "        \n",
    "# List all files in dataset directory\n",
    "if os.path.exists(dataset_dir):\n",
    "    print(f\"\\nüìÅ Dataset directory contents:\")\n",
    "    for file in os.listdir(dataset_dir):\n",
    "        file_path = os.path.join(dataset_dir, file)\n",
    "        if os.path.isfile(file_path):\n",
    "            file_size = os.path.getsize(file_path) / 1024 / 1024\n",
    "            print(f\"   - {file} ({file_size:.1f} MB)\")\n",
    "        else:\n",
    "            print(f\"   - {file}/ (directory)\")\n",
    "else:\n",
    "    print(f\"‚ùå Dataset directory not found: {dataset_dir}\")\n",
    "    print(\"üìã Available input directories:\")\n",
    "    for item in os.listdir(\"/kaggle/input\"):\n",
    "        print(f\"   - /kaggle/input/{item}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2522788",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pickle datasets\n",
    "print(\"üìÇ Loading pickle datasets...\")\n",
    "\n",
    "with open(train_pkl_path, 'rb') as f:\n",
    "    train_data = pickle.load(f)\n",
    "    \n",
    "with open(val_pkl_path, 'rb') as f:\n",
    "    val_data = pickle.load(f)\n",
    "    \n",
    "with open(test_pkl_path, 'rb') as f:\n",
    "    test_data = pickle.load(f)\n",
    "\n",
    "print(f\"‚úÖ Datasets loaded successfully:\")\n",
    "print(f\"   - Training: {len(train_data):,} samples\")\n",
    "print(f\"   - Validation: {len(val_data):,} samples\")\n",
    "print(f\"   - Test: {len(test_data):,} samples\")\n",
    "print(f\"   - Total: {len(train_data) + len(val_data) + len(test_data):,} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5770d877",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine data structure\n",
    "print(\"üîç Examining data structure...\")\n",
    "\n",
    "if len(train_data) > 0:\n",
    "    sample = train_data[0]\n",
    "    print(f\"\\nüìã Sample keys: {list(sample.keys())}\")\n",
    "    \n",
    "    # Check if data is pre-tokenized or raw text\n",
    "    if 'input_ids' in sample:\n",
    "        print(\"‚úÖ Data is pre-tokenized\")\n",
    "        print(f\"   - Input IDs length: {len(sample['input_ids'])}\")\n",
    "        print(f\"   - Has attention mask: {'attention_mask' in sample}\")\n",
    "        print(f\"   - Has labels: {'labels' in sample}\")\n",
    "    else:\n",
    "        print(\"üìù Data contains raw text\")\n",
    "        for key in sample.keys():\n",
    "            if isinstance(sample[key], str):\n",
    "                print(f\"   - {key}: {len(sample[key])} characters\")\n",
    "                print(f\"     Preview: {sample[key][:100]}...\")\n",
    "            else:\n",
    "                print(f\"   - {key}: {type(sample[key])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff68ef8",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing and Feature Engineering\n",
    "\n",
    "Setting up the PyTorch Dataset class and data preprocessing pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "814ed59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration for Kaggle P100\n",
    "@dataclass\n",
    "class KaggleTrainingConfig:\n",
    "    \"\"\"Optimized configuration for Kaggle P100 GPU (16GB VRAM) - Balanced Speed & Memory\"\"\"\n",
    "    \n",
    "    # Model configuration\n",
    "    model_name: str = \"Qwen/Qwen2.5-Coder-3B-Instruct\"\n",
    "    max_length: int = 384  # Further reduced for speed\n",
    "    \n",
    "    # Training parameters - Balanced for speed\n",
    "    train_batch_size: int = 2  # Increased from 1 to 2\n",
    "    eval_batch_size: int = 4   \n",
    "    gradient_accumulation_steps: int = 8  # Reduced from 16 to 8\n",
    "    num_epochs: int = 2  # Reduced from 3 to 2 for faster completion\n",
    "    learning_rate: float = 2e-4\n",
    "    weight_decay: float = 0.001\n",
    "    warmup_ratio: float = 0.03\n",
    "    \n",
    "    # LoRA configuration - Keep memory-optimized\n",
    "    lora_r: int = 8        \n",
    "    lora_alpha: int = 16   \n",
    "    lora_dropout: float = 0.05\n",
    "    target_modules: List[str] = None\n",
    "    \n",
    "    # Output configuration\n",
    "    output_dir: str = \"/kaggle/working/qwen-coder-finetune\"\n",
    "    run_name: str = \"qwen-coder-unittest-kaggle\"\n",
    "    logging_steps: int = 5  # More frequent logging\n",
    "    save_steps: int = 250   # More frequent saves\n",
    "    eval_steps: int = 250   \n",
    "    \n",
    "    # Hardware optimization for P100\n",
    "    use_cuda: bool = True\n",
    "    mixed_precision: bool = True\n",
    "    gradient_checkpointing: bool = True  \n",
    "    \n",
    "    def __post_init__(self):\n",
    "        if self.target_modules is None:\n",
    "            # Keep reduced target modules \n",
    "            self.target_modules = [\n",
    "                \"q_proj\", \"v_proj\", \"o_proj\",\n",
    "                \"gate_proj\", \"down_proj\"\n",
    "            ]\n",
    "\n",
    "# Initialize configuration\n",
    "config = KaggleTrainingConfig()\n",
    "print(f\"üìã Training Configuration (Speed Optimized):\")\n",
    "print(f\"   - Model: {config.model_name}\")\n",
    "print(f\"   - Epochs: {config.num_epochs}\")\n",
    "print(f\"   - Batch size: {config.train_batch_size}\")\n",
    "print(f\"   - Effective batch size: {config.train_batch_size * config.gradient_accumulation_steps}\")\n",
    "print(f\"   - Learning rate: {config.learning_rate}\")\n",
    "print(f\"   - Max length: {config.max_length}\")\n",
    "print(f\"   - LoRA rank: {config.lora_r}\")\n",
    "print(f\"   - Target modules: {len(config.target_modules)} modules\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2391cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch Dataset class for unit test generation\n",
    "class UnitTestDataset(Dataset):\n",
    "    \"\"\"PyTorch Dataset for unit test generation training\"\"\"\n",
    "    \n",
    "    def __init__(self, data: List[Dict], tokenizer, max_length: int = 1024):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        print(f\"Dataset initialized with {len(data):,} samples\")\n",
    "        if len(data) > 0:\n",
    "            print(f\"Sample keys: {list(data[0].keys())}\")\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:\n",
    "        item = self.data[idx]\n",
    "        \n",
    "        # Handle pre-tokenized data\n",
    "        if 'input_ids' in item and 'labels' in item:\n",
    "            return {\n",
    "                'input_ids': torch.tensor(item['input_ids'][:self.max_length], dtype=torch.long),\n",
    "                'attention_mask': torch.tensor(item['attention_mask'][:self.max_length], dtype=torch.long),\n",
    "                'labels': torch.tensor(item['labels'][:self.max_length], dtype=torch.long)\n",
    "            }\n",
    "        \n",
    "        # Handle raw text data\n",
    "        else:\n",
    "            code = item.get('code', '')\n",
    "            unit_test = item.get('unit_test', '')\n",
    "            \n",
    "            # Create training prompt\n",
    "            prompt = f\"# Generate a unit test for the following Python function:\\n{code}\\n\\n# Unit test:\\n{unit_test}\"\n",
    "            \n",
    "            # Tokenize\n",
    "            encoded = self.tokenizer(\n",
    "                prompt,\n",
    "                truncation=True,\n",
    "                padding='max_length',\n",
    "                max_length=self.max_length,\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "            \n",
    "            return {\n",
    "                'input_ids': encoded['input_ids'].squeeze(),\n",
    "                'attention_mask': encoded['attention_mask'].squeeze(),\n",
    "                'labels': encoded['input_ids'].squeeze().clone()\n",
    "            }\n",
    "\n",
    "print(\"‚úÖ UnitTestDataset class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa73ebb",
   "metadata": {},
   "source": [
    "## 4. Model Architecture Setup\n",
    "\n",
    "Loading the Qwen2.5-Coder model with quantization and setting up LoRA fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c058c5bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility functions\n",
    "def clear_memory():\n",
    "    \"\"\"Clear GPU and system memory\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.synchronize()\n",
    "    gc.collect()\n",
    "\n",
    "def check_gpu_memory():\n",
    "    \"\"\"Check and print GPU memory usage\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        allocated = torch.cuda.memory_allocated() / 1024**3\n",
    "        reserved = torch.cuda.memory_reserved() / 1024**3\n",
    "        print(f\"üñ•Ô∏è  GPU Memory - Allocated: {allocated:.2f}GB, Reserved: {reserved:.2f}GB\")\n",
    "    else:\n",
    "        print(\"‚ùå CUDA not available\")\n",
    "\n",
    "def create_quantization_config() -> BitsAndBytesConfig:\n",
    "    \"\"\"Create 4-bit quantization configuration\"\"\"\n",
    "    return BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16\n",
    "    )\n",
    "\n",
    "print(\"‚úÖ Utility functions defined\")\n",
    "clear_memory()\n",
    "check_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2458f78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer\n",
    "print(\"ü§ñ Loading tokenizer...\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    config.model_name,\n",
    "    trust_remote_code=True,\n",
    "    padding_side=\"left\"\n",
    ")\n",
    "\n",
    "# Add pad token if missing\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    print(\"‚úÖ Added pad token\")\n",
    "\n",
    "print(f\"‚úÖ Tokenizer loaded: {config.model_name}\")\n",
    "print(f\"   - Vocab size: {len(tokenizer)}\")\n",
    "print(f\"   - Special tokens: pad={tokenizer.pad_token}, eos={tokenizer.eos_token}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "294df842",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model with quantization\n",
    "print(\"ü§ñ Loading model with quantization...\")\n",
    "\n",
    "quantization_config = create_quantization_config()\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    config.model_name,\n",
    "    quantization_config=quantization_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    attn_implementation=\"eager\"  # Avoid flash attention compatibility issues\n",
    ")\n",
    "\n",
    "# Resize token embeddings if needed\n",
    "if len(tokenizer) != model.config.vocab_size:\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    print(f\"   - Resized token embeddings to {len(tokenizer)}\")\n",
    "\n",
    "print(f\"‚úÖ Model loaded: {config.model_name}\")\n",
    "print(f\"   - Parameters: {model.num_parameters():,}\")\n",
    "print(f\"   - Vocab size: {model.config.vocab_size}\")\n",
    "check_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc0204fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup LoRA fine-tuning\n",
    "print(\"üîß Setting up LoRA configuration...\")\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=config.lora_r,\n",
    "    lora_alpha=config.lora_alpha,\n",
    "    target_modules=config.target_modules,\n",
    "    lora_dropout=config.lora_dropout,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    ")\n",
    "\n",
    "# Apply LoRA to the model\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Enable gradient computation for LoRA parameters\n",
    "model.train()\n",
    "\n",
    "# Ensure LoRA parameters require gradients\n",
    "for name, param in model.named_parameters():\n",
    "    if 'lora_' in name:\n",
    "        param.requires_grad = True\n",
    "\n",
    "# Prepare model for training\n",
    "model.enable_input_require_grads()\n",
    "\n",
    "# Print trainable parameters\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "\n",
    "print(f\"‚úÖ LoRA setup complete:\")\n",
    "print(f\"   - Trainable params: {trainable_params:,}\")\n",
    "print(f\"   - Total params: {total_params:,}\")\n",
    "print(f\"   - Trainable percentage: {100 * trainable_params / total_params:.2f}%\")\n",
    "\n",
    "# Verify gradient requirements\n",
    "lora_params_with_grad = sum(1 for name, param in model.named_parameters() \n",
    "                           if 'lora_' in name and param.requires_grad)\n",
    "print(f\"   - LoRA params with gradients: {lora_params_with_grad}\")\n",
    "\n",
    "check_gpu_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69837aef",
   "metadata": {},
   "source": [
    "## 5. Training Configuration and Hyperparameters\n",
    "\n",
    "Setting up training arguments and creating PyTorch datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d47688a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create PyTorch datasets\n",
    "print(\"üìä Creating PyTorch datasets...\")\n",
    "\n",
    "train_dataset = UnitTestDataset(train_data, tokenizer, config.max_length)\n",
    "val_dataset = UnitTestDataset(val_data, tokenizer, config.max_length)\n",
    "test_dataset = UnitTestDataset(test_data, tokenizer, config.max_length)\n",
    "\n",
    "print(f\"‚úÖ Datasets created:\")\n",
    "print(f\"   - Training: {len(train_dataset):,} samples\")\n",
    "print(f\"   - Validation: {len(val_dataset):,} samples\")\n",
    "print(f\"   - Test: {len(test_dataset):,} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ccfe6b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data collator\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,\n",
    "    pad_to_multiple_of=8\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Data collator created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d883b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=config.output_dir,\n",
    "    run_name=config.run_name,\n",
    "    \n",
    "    # Training parameters\n",
    "    num_train_epochs=config.num_epochs,\n",
    "    per_device_train_batch_size=config.train_batch_size,\n",
    "    per_device_eval_batch_size=config.eval_batch_size,\n",
    "    gradient_accumulation_steps=config.gradient_accumulation_steps,\n",
    "    \n",
    "    # Optimization\n",
    "    learning_rate=config.learning_rate,\n",
    "    weight_decay=config.weight_decay,\n",
    "    warmup_ratio=config.warmup_ratio,\n",
    "    \n",
    "    # Hardware optimization for P100\n",
    "    fp16=False,\n",
    "    bf16=config.mixed_precision,\n",
    "    gradient_checkpointing=config.gradient_checkpointing,\n",
    "    dataloader_pin_memory=False,\n",
    "    \n",
    "    # Logging and saving\n",
    "    logging_steps=config.logging_steps,\n",
    "    eval_steps=config.eval_steps,\n",
    "    save_steps=config.save_steps,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    \n",
    "    # Model selection\n",
    "    remove_unused_columns=False,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    \n",
    "    # Disable external logging\n",
    "    report_to=[]\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Training arguments created\")\n",
    "print(f\"   - Total training steps: {len(train_dataset) // (config.train_batch_size * config.gradient_accumulation_steps) * config.num_epochs}\")\n",
    "print(f\"   - Steps per epoch: {len(train_dataset) // (config.train_batch_size * config.gradient_accumulation_steps)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd306675",
   "metadata": {},
   "source": [
    "## 6. Model Training Loop\n",
    "\n",
    "Running the complete training pipeline with progress monitoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb7d4ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create trainer\n",
    "print(\"üèãÔ∏è Creating trainer...\")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Trainer created successfully\")\n",
    "check_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868e6384",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training\n",
    "print(\"\\nüî• Starting training...\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"üìä Training Configuration Summary:\")\n",
    "print(f\"   - Model: {config.model_name}\")\n",
    "print(f\"   - Training samples: {len(train_dataset):,}\")\n",
    "print(f\"   - Validation samples: {len(val_dataset):,}\")\n",
    "print(f\"   - Epochs: {config.num_epochs}\")\n",
    "print(f\"   - Batch size: {config.train_batch_size} (effective: {config.train_batch_size * config.gradient_accumulation_steps})\")\n",
    "print(f\"   - Learning rate: {config.learning_rate}\")\n",
    "print(f\"   - Max sequence length: {config.max_length}\")\n",
    "print(f\"   - Steps per epoch: {len(train_dataset) // (config.train_batch_size * config.gradient_accumulation_steps)}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Aggressive memory cleanup before training\n",
    "print(\"üßπ Performing aggressive memory cleanup...\")\n",
    "clear_memory()\n",
    "\n",
    "# Set CUDA memory allocation configuration for better memory management\n",
    "import os\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "\n",
    "check_gpu_memory()\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "try:\n",
    "    # Train the model\n",
    "    trainer.train()\n",
    "    \n",
    "    training_time = time.time() - start_time\n",
    "    print(f\"\\n‚úÖ Training completed successfully!\")\n",
    "    print(f\"‚è±Ô∏è  Total training time: {training_time / 3600:.2f} hours ({training_time / 60:.1f} minutes)\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Training failed: {str(e)}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    \n",
    "finally:\n",
    "    clear_memory()\n",
    "    check_gpu_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6dd2da",
   "metadata": {},
   "source": [
    "## 7. Model Evaluation and Testing\n",
    "\n",
    "Evaluating model performance and generating sample unit tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af8c9bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final evaluation on test set\n",
    "print(\"üìä Running final evaluation on test set...\")\n",
    "\n",
    "eval_results = trainer.evaluate(eval_dataset=test_dataset)\n",
    "\n",
    "print(f\"\\nüìà Final Evaluation Results:\")\n",
    "print(f\"   - Test Loss: {eval_results['eval_loss']:.4f}\")\n",
    "print(f\"   - Test Perplexity: {np.exp(eval_results['eval_loss']):.2f}\")\n",
    "\n",
    "# Save evaluation results\n",
    "with open(f\"{config.output_dir}/eval_results.json\", \"w\") as f:\n",
    "    json.dump(eval_results, f, indent=2)\n",
    "    \n",
    "print(f\"‚úÖ Evaluation results saved to {config.output_dir}/eval_results.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476fcf11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test model with sample generations\n",
    "def test_model_generation(model, tokenizer, test_data: List[Dict], num_samples: int = 5):\n",
    "    \"\"\"Test the trained model with sample unit test generations\"\"\"\n",
    "    print(f\"üß™ Testing model with {num_samples} sample generations...\\n\")\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    # Select random test samples\n",
    "    test_samples = np.random.choice(test_data, min(num_samples, len(test_data)), replace=False)\n",
    "    \n",
    "    for i, sample in enumerate(test_samples):\n",
    "        print(f\"{'='*80}\")\n",
    "        print(f\"Test Sample {i+1}/{num_samples}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        # Extract code from sample\n",
    "        if 'code' in sample:\n",
    "            code = sample['code']\n",
    "        else:\n",
    "            # Try to extract from tokenized data\n",
    "            input_ids = sample.get('input_ids', [])\n",
    "            if input_ids:\n",
    "                decoded = tokenizer.decode(input_ids[:200], skip_special_tokens=True)\n",
    "                # Extract code portion (simplified)\n",
    "                code = decoded.split('# Unit test:')[0].replace('# Generate a unit test for the following Python function:', '').strip()\n",
    "            else:\n",
    "                code = \"Sample code not available\"\n",
    "        \n",
    "        print(f\"üìù Original Code:\")\n",
    "        print(f\"{code[:300]}{'...' if len(code) > 300 else ''}\\n\")\n",
    "        \n",
    "        # Create generation prompt\n",
    "        prompt = f\"# Generate a unit test for the following Python function:\\n{code}\\n\\n# Unit test:\\n\"\n",
    "        \n",
    "        # Tokenize prompt\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "        inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "        \n",
    "        # Generate unit test\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=300,\n",
    "                temperature=0.7,\n",
    "                do_sample=True,\n",
    "                top_p=0.9,\n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "                eos_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "        \n",
    "        # Decode generated response\n",
    "        full_response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        generated_test = full_response[len(prompt):].strip()\n",
    "        \n",
    "        print(f\"ü§ñ Generated Unit Test:\")\n",
    "        print(f\"{generated_test}\\n\")\n",
    "        \n",
    "        # Show original unit test if available\n",
    "        if 'unit_test' in sample:\n",
    "            print(f\"‚úÖ Original Unit Test:\")\n",
    "            print(f\"{sample['unit_test'][:300]}{'...' if len(sample['unit_test']) > 300 else ''}\\n\")\n",
    "        \n",
    "        print(\"\\n\")\n",
    "\n",
    "# Run sample generations\n",
    "test_model_generation(model, tokenizer, test_data, num_samples=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd6bda1",
   "metadata": {},
   "source": [
    "## 8. Save Trained Model\n",
    "\n",
    "Saving the fine-tuned model and creating downloadable archives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63868a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "print(\"üíæ Saving trained model...\")\n",
    "\n",
    "# Save model and tokenizer\n",
    "trainer.save_model(config.output_dir)\n",
    "tokenizer.save_pretrained(config.output_dir)\n",
    "\n",
    "print(f\"‚úÖ Model saved to: {config.output_dir}\")\n",
    "\n",
    "# Save training configuration\n",
    "config_dict = {\n",
    "    'model_name': config.model_name,\n",
    "    'max_length': config.max_length,\n",
    "    'train_batch_size': config.train_batch_size,\n",
    "    'gradient_accumulation_steps': config.gradient_accumulation_steps,\n",
    "    'num_epochs': config.num_epochs,\n",
    "    'learning_rate': config.learning_rate,\n",
    "    'lora_r': config.lora_r,\n",
    "    'lora_alpha': config.lora_alpha,\n",
    "    'lora_dropout': config.lora_dropout,\n",
    "    'target_modules': config.target_modules,\n",
    "    'training_samples': len(train_dataset),\n",
    "    'validation_samples': len(val_dataset),\n",
    "    'test_samples': len(test_dataset)\n",
    "}\n",
    "\n",
    "with open(f\"{config.output_dir}/training_config.json\", \"w\") as f:\n",
    "    json.dump(config_dict, f, indent=2)\n",
    "\n",
    "print(f\"‚úÖ Training configuration saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e872c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List saved files\n",
    "print(\"üìÅ Saved model files:\")\n",
    "for file in os.listdir(config.output_dir):\n",
    "    file_path = os.path.join(config.output_dir, file)\n",
    "    if os.path.isfile(file_path):\n",
    "        size_mb = os.path.getsize(file_path) / 1024 / 1024\n",
    "        print(f\"   - {file}: {size_mb:.1f} MB\")\n",
    "    else:\n",
    "        print(f\"   - {file}/ (directory)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af559eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create downloadable archive\n",
    "print(\"üì¶ Creating downloadable model archive...\")\n",
    "\n",
    "archive_name = \"qwen-coder-unittest-model\"\n",
    "!cd /kaggle/working && tar -czf {archive_name}.tar.gz qwen-coder-finetune/\n",
    "\n",
    "# Check archive size\n",
    "archive_path = f\"/kaggle/working/{archive_name}.tar.gz\"\n",
    "if os.path.exists(archive_path):\n",
    "    archive_size = os.path.getsize(archive_path) / 1024 / 1024\n",
    "    print(f\"‚úÖ Model archive created: {archive_name}.tar.gz ({archive_size:.1f} MB)\")\n",
    "    print(f\"üì• Download from: /kaggle/working/{archive_name}.tar.gz\")\n",
    "else:\n",
    "    print(f\"‚ùå Failed to create archive\")\n",
    "\n",
    "# Final memory cleanup\n",
    "clear_memory()\n",
    "print(\"\\nüéâ Training pipeline completed successfully!\")\n",
    "print(f\"üìÅ Model saved in: {config.output_dir}\")\n",
    "print(f\"üì¶ Download archive: {archive_name}.tar.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ab514d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training summary\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üéØ TRAINING SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(f\"üìä Dataset: {len(train_data) + len(val_data) + len(test_data):,} total samples\")\n",
    "print(f\"ü§ñ Model: {config.model_name}\")\n",
    "print(f\"üîß Method: LoRA fine-tuning with 4-bit quantization\")\n",
    "print(f\"‚ö° Hardware: Kaggle P100 GPU\")\n",
    "print(f\"üìà Training: {config.num_epochs} epochs, {len(train_dataset):,} samples\")\n",
    "print(f\"üíæ Output: {config.output_dir}\")\n",
    "print(f\"üì¶ Archive: {archive_name}.tar.gz\")\n",
    "print(\"\\n‚úÖ Ready for deployment and inference!\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
