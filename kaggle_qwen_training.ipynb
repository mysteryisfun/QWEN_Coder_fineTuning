{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1821982e",
   "metadata": {},
   "source": [
    "# Qwen2.5-Coder Fine-tuning on Kaggle P100 GPU\n",
    "\n",
    "Complete end-to-end fine-tuning pipeline for Qwen2.5-Coder-3B to generate Python unit tests.\n",
    "\n",
    "**Dataset**: https://www.kaggle.com/datasets/ujwalsr/finetuning\n",
    "**GPU**: Kaggle P100 (16GB VRAM)\n",
    "**Method**: LoRA fine-tuning with 4-bit quantization\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225ad047",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries\n",
    "\n",
    "Installing and importing all necessary dependencies for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb79690",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q transformers>=4.36.0\n",
    "!pip install -q peft>=0.7.0\n",
    "!pip install -q bitsandbytes>=0.41.0\n",
    "!pip install -q accelerate>=0.24.0\n",
    "!pip install -q datasets\n",
    "!pip install -q tqdm\n",
    "\n",
    "print(\"âœ… Packages installed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8fd43e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import essential libraries\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import gc\n",
    "import time\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Any, Optional\n",
    "from dataclasses import dataclass\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Transformers and PEFT\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForCausalLM, \n",
    "    TrainingArguments, Trainer,\n",
    "    BitsAndBytesConfig, DataCollatorForLanguageModeling\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Check GPU availability\n",
    "print(f\"ğŸ–¥ï¸  CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"   GPU: {torch.cuda.get_device_name()}\")\n",
    "    print(f\"   GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f}GB\")\n",
    "    \n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b2eb68b",
   "metadata": {},
   "source": [
    "## 2. Load and Prepare Dataset\n",
    "\n",
    "Loading the three pickle files from the Kaggle dataset and examining the data structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8481b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# Dataset paths (Kaggle input directory)\n",
    "dataset_dir = \"/kaggle/input/finetuning\"\n",
    "\n",
    "train_pkl_path = f\"{dataset_dir}/train_dataset.pkl\"\n",
    "val_pkl_path = f\"{dataset_dir}/val_dataset.pkl\"\n",
    "test_pkl_path = f\"{dataset_dir}/test_dataset.pkl\"\n",
    "\n",
    "# Verify files exist\n",
    "print(f\"\\nğŸ” Checking for dataset files in: {dataset_dir}\")\n",
    "for path in [train_pkl_path, val_pkl_path, test_pkl_path]:\n",
    "    if os.path.exists(path):\n",
    "        file_size = os.path.getsize(path) / 1024 / 1024  # Size in MB\n",
    "        print(f\"âœ… Found: {path} ({file_size:.1f} MB)\")\n",
    "    else:\n",
    "        print(f\"âŒ Missing: {path}\")\n",
    "        \n",
    "# List all files in dataset directory\n",
    "if os.path.exists(dataset_dir):\n",
    "    print(f\"\\nğŸ“ Dataset directory contents:\")\n",
    "    for file in os.listdir(dataset_dir):\n",
    "        file_path = os.path.join(dataset_dir, file)\n",
    "        if os.path.isfile(file_path):\n",
    "            file_size = os.path.getsize(file_path) / 1024 / 1024\n",
    "            print(f\"   - {file} ({file_size:.1f} MB)\")\n",
    "        else:\n",
    "            print(f\"   - {file}/ (directory)\")\n",
    "else:\n",
    "    print(f\"âŒ Dataset directory not found: {dataset_dir}\")\n",
    "    print(\"ğŸ“‹ Available input directories:\")\n",
    "    for item in os.listdir(\"/kaggle/input\"):\n",
    "        print(f\"   - /kaggle/input/{item}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2522788",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pickle datasets\n",
    "print(\"ğŸ“‚ Loading pickle datasets...\")\n",
    "\n",
    "with open(train_pkl_path, 'rb') as f:\n",
    "    train_data = pickle.load(f)\n",
    "    \n",
    "with open(val_pkl_path, 'rb') as f:\n",
    "    val_data = pickle.load(f)\n",
    "    \n",
    "with open(test_pkl_path, 'rb') as f:\n",
    "    test_data = pickle.load(f)\n",
    "\n",
    "print(f\"âœ… Datasets loaded successfully:\")\n",
    "print(f\"   - Training: {len(train_data):,} samples\")\n",
    "print(f\"   - Validation: {len(val_data):,} samples\")\n",
    "print(f\"   - Test: {len(test_data):,} samples\")\n",
    "print(f\"   - Total: {len(train_data) + len(val_data) + len(test_data):,} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5770d877",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine data structure\n",
    "print(\"ğŸ” Examining data structure...\")\n",
    "\n",
    "if len(train_data) > 0:\n",
    "    sample = train_data[0]\n",
    "    print(f\"\\nğŸ“‹ Sample keys: {list(sample.keys())}\")\n",
    "    \n",
    "    # Check if data is pre-tokenized or raw text\n",
    "    if 'input_ids' in sample:\n",
    "        print(\"âœ… Data is pre-tokenized\")\n",
    "        print(f\"   - Input IDs length: {len(sample['input_ids'])}\")\n",
    "        print(f\"   - Has attention mask: {'attention_mask' in sample}\")\n",
    "        print(f\"   - Has labels: {'labels' in sample}\")\n",
    "    else:\n",
    "        print(\"ğŸ“ Data contains raw text\")\n",
    "        for key in sample.keys():\n",
    "            if isinstance(sample[key], str):\n",
    "                print(f\"   - {key}: {len(sample[key])} characters\")\n",
    "                print(f\"     Preview: {sample[key][:100]}...\")\n",
    "            else:\n",
    "                print(f\"   - {key}: {type(sample[key])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff68ef8",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing and Feature Engineering\n",
    "\n",
    "Setting up the PyTorch Dataset class and data preprocessing pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "814ed59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration for Kaggle P100\n",
    "@dataclass\n",
    "class KaggleTrainingConfig:\n",
    "    \"\"\"Optimized configuration for Kaggle P100 GPU (16GB VRAM)\"\"\"\n",
    "    \n",
    "    # Model configuration\n",
    "    model_name: str = \"Qwen/Qwen2.5-Coder-3B-Instruct\"\n",
    "    max_length: int = 1024  # P100 can handle longer sequences\n",
    "    \n",
    "    # Training parameters - P100 optimized\n",
    "    train_batch_size: int = 4\n",
    "    eval_batch_size: int = 8\n",
    "    gradient_accumulation_steps: int = 4  # Effective batch size: 16\n",
    "    num_epochs: int = 3\n",
    "    learning_rate: float = 2e-4\n",
    "    weight_decay: float = 0.001\n",
    "    warmup_ratio: float = 0.03\n",
    "    \n",
    "    # LoRA configuration\n",
    "    lora_r: int = 16\n",
    "    lora_alpha: int = 32\n",
    "    lora_dropout: float = 0.05\n",
    "    target_modules: List[str] = None\n",
    "    \n",
    "    # Output configuration\n",
    "    output_dir: str = \"/kaggle/working/qwen-coder-finetune\"\n",
    "    run_name: str = \"qwen-coder-unittest-kaggle\"\n",
    "    logging_steps: int = 10\n",
    "    save_steps: int = 500\n",
    "    eval_steps: int = 500\n",
    "    \n",
    "    # Hardware optimization for P100\n",
    "    use_cuda: bool = True\n",
    "    mixed_precision: bool = True\n",
    "    gradient_checkpointing: bool = True  # Enable for P100\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        if self.target_modules is None:\n",
    "            self.target_modules = [\n",
    "                \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                \"gate_proj\", \"up_proj\", \"down_proj\"\n",
    "            ]\n",
    "\n",
    "# Initialize configuration\n",
    "config = KaggleTrainingConfig()\n",
    "print(f\"ğŸ“‹ Training Configuration:\")\n",
    "print(f\"   - Model: {config.model_name}\")\n",
    "print(f\"   - Epochs: {config.num_epochs}\")\n",
    "print(f\"   - Batch size: {config.train_batch_size}\")\n",
    "print(f\"   - Effective batch size: {config.train_batch_size * config.gradient_accumulation_steps}\")\n",
    "print(f\"   - Learning rate: {config.learning_rate}\")\n",
    "print(f\"   - Max length: {config.max_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2391cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch Dataset class for unit test generation\n",
    "class UnitTestDataset(Dataset):\n",
    "    \"\"\"PyTorch Dataset for unit test generation training\"\"\"\n",
    "    \n",
    "    def __init__(self, data: List[Dict], tokenizer, max_length: int = 1024):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        print(f\"Dataset initialized with {len(data):,} samples\")\n",
    "        if len(data) > 0:\n",
    "            print(f\"Sample keys: {list(data[0].keys())}\")\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:\n",
    "        item = self.data[idx]\n",
    "        \n",
    "        # Handle pre-tokenized data\n",
    "        if 'input_ids' in item and 'labels' in item:\n",
    "            return {\n",
    "                'input_ids': torch.tensor(item['input_ids'][:self.max_length], dtype=torch.long),\n",
    "                'attention_mask': torch.tensor(item['attention_mask'][:self.max_length], dtype=torch.long),\n",
    "                'labels': torch.tensor(item['labels'][:self.max_length], dtype=torch.long)\n",
    "            }\n",
    "        \n",
    "        # Handle raw text data\n",
    "        else:\n",
    "            code = item.get('code', '')\n",
    "            unit_test = item.get('unit_test', '')\n",
    "            \n",
    "            # Create training prompt\n",
    "            prompt = f\"# Generate a unit test for the following Python function:\\n{code}\\n\\n# Unit test:\\n{unit_test}\"\n",
    "            \n",
    "            # Tokenize\n",
    "            encoded = self.tokenizer(\n",
    "                prompt,\n",
    "                truncation=True,\n",
    "                padding='max_length',\n",
    "                max_length=self.max_length,\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "            \n",
    "            return {\n",
    "                'input_ids': encoded['input_ids'].squeeze(),\n",
    "                'attention_mask': encoded['attention_mask'].squeeze(),\n",
    "                'labels': encoded['input_ids'].squeeze().clone()\n",
    "            }\n",
    "\n",
    "print(\"âœ… UnitTestDataset class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa73ebb",
   "metadata": {},
   "source": [
    "## 4. Model Architecture Setup\n",
    "\n",
    "Loading the Qwen2.5-Coder model with quantization and setting up LoRA fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c058c5bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility functions\n",
    "def clear_memory():\n",
    "    \"\"\"Clear GPU and system memory\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.synchronize()\n",
    "    gc.collect()\n",
    "\n",
    "def check_gpu_memory():\n",
    "    \"\"\"Check and print GPU memory usage\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        allocated = torch.cuda.memory_allocated() / 1024**3\n",
    "        reserved = torch.cuda.memory_reserved() / 1024**3\n",
    "        print(f\"ğŸ–¥ï¸  GPU Memory - Allocated: {allocated:.2f}GB, Reserved: {reserved:.2f}GB\")\n",
    "    else:\n",
    "        print(\"âŒ CUDA not available\")\n",
    "\n",
    "def create_quantization_config() -> BitsAndBytesConfig:\n",
    "    \"\"\"Create 4-bit quantization configuration\"\"\"\n",
    "    return BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16\n",
    "    )\n",
    "\n",
    "print(\"âœ… Utility functions defined\")\n",
    "clear_memory()\n",
    "check_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2458f78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer\n",
    "print(\"ğŸ¤– Loading tokenizer...\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    config.model_name,\n",
    "    trust_remote_code=True,\n",
    "    padding_side=\"left\"\n",
    ")\n",
    "\n",
    "# Add pad token if missing\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    print(\"âœ… Added pad token\")\n",
    "\n",
    "print(f\"âœ… Tokenizer loaded: {config.model_name}\")\n",
    "print(f\"   - Vocab size: {len(tokenizer)}\")\n",
    "print(f\"   - Special tokens: pad={tokenizer.pad_token}, eos={tokenizer.eos_token}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "294df842",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model with quantization\n",
    "print(\"ğŸ¤– Loading model with quantization...\")\n",
    "\n",
    "quantization_config = create_quantization_config()\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    config.model_name,\n",
    "    quantization_config=quantization_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    attn_implementation=\"eager\"  # Avoid flash attention compatibility issues\n",
    ")\n",
    "\n",
    "print(f\"âœ… Model loaded: {config.model_name}\")\n",
    "print(f\"   - Parameters: {model.num_parameters():,}\")\n",
    "check_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc0204fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup LoRA fine-tuning\n",
    "print(\"ğŸ”§ Setting up LoRA configuration...\")\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=config.lora_r,\n",
    "    lora_alpha=config.lora_alpha,\n",
    "    target_modules=config.target_modules,\n",
    "    lora_dropout=config.lora_dropout,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Print trainable parameters\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "\n",
    "print(f\"âœ… LoRA setup complete:\")\n",
    "print(f\"   - Trainable params: {trainable_params:,}\")\n",
    "print(f\"   - Total params: {total_params:,}\")\n",
    "print(f\"   - Trainable percentage: {100 * trainable_params / total_params:.2f}%\")\n",
    "\n",
    "check_gpu_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69837aef",
   "metadata": {},
   "source": [
    "## 5. Training Configuration and Hyperparameters\n",
    "\n",
    "Setting up training arguments and creating PyTorch datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d47688a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create PyTorch datasets\n",
    "print(\"ğŸ“Š Creating PyTorch datasets...\")\n",
    "\n",
    "train_dataset = UnitTestDataset(train_data, tokenizer, config.max_length)\n",
    "val_dataset = UnitTestDataset(val_data, tokenizer, config.max_length)\n",
    "test_dataset = UnitTestDataset(test_data, tokenizer, config.max_length)\n",
    "\n",
    "print(f\"âœ… Datasets created:\")\n",
    "print(f\"   - Training: {len(train_dataset):,} samples\")\n",
    "print(f\"   - Validation: {len(val_dataset):,} samples\")\n",
    "print(f\"   - Test: {len(test_dataset):,} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ccfe6b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data collator\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,\n",
    "    pad_to_multiple_of=8\n",
    ")\n",
    "\n",
    "print(\"âœ… Data collator created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d883b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=config.output_dir,\n",
    "    run_name=config.run_name,\n",
    "    \n",
    "    # Training parameters\n",
    "    num_train_epochs=config.num_epochs,\n",
    "    per_device_train_batch_size=config.train_batch_size,\n",
    "    per_device_eval_batch_size=config.eval_batch_size,\n",
    "    gradient_accumulation_steps=config.gradient_accumulation_steps,\n",
    "    \n",
    "    # Optimization\n",
    "    learning_rate=config.learning_rate,\n",
    "    weight_decay=config.weight_decay,\n",
    "    warmup_ratio=config.warmup_ratio,\n",
    "    \n",
    "    # Hardware optimization for P100\n",
    "    fp16=False,\n",
    "    bf16=config.mixed_precision,\n",
    "    gradient_checkpointing=config.gradient_checkpointing,\n",
    "    dataloader_pin_memory=False,\n",
    "    \n",
    "    # Logging and saving\n",
    "    logging_steps=config.logging_steps,\n",
    "    eval_steps=config.eval_steps,\n",
    "    save_steps=config.save_steps,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    \n",
    "    # Model selection\n",
    "    remove_unused_columns=False,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    \n",
    "    # Disable external logging\n",
    "    report_to=[]\n",
    ")\n",
    "\n",
    "print(\"âœ… Training arguments created\")\n",
    "print(f\"   - Total training steps: {len(train_dataset) // (config.train_batch_size * config.gradient_accumulation_steps) * config.num_epochs}\")\n",
    "print(f\"   - Steps per epoch: {len(train_dataset) // (config.train_batch_size * config.gradient_accumulation_steps)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd306675",
   "metadata": {},
   "source": [
    "## 6. Model Training Loop\n",
    "\n",
    "Running the complete training pipeline with progress monitoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb7d4ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create trainer\n",
    "print(\"ğŸ‹ï¸ Creating trainer...\")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "print(\"âœ… Trainer created successfully\")\n",
    "check_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868e6384",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training\n",
    "print(\"\\nğŸ”¥ Starting training...\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"ğŸ“Š Training Configuration Summary:\")\n",
    "print(f\"   - Model: {config.model_name}\")\n",
    "print(f\"   - Training samples: {len(train_dataset):,}\")\n",
    "print(f\"   - Validation samples: {len(val_dataset):,}\")\n",
    "print(f\"   - Epochs: {config.num_epochs}\")\n",
    "print(f\"   - Batch size: {config.train_batch_size} (effective: {config.train_batch_size * config.gradient_accumulation_steps})\")\n",
    "print(f\"   - Learning rate: {config.learning_rate}\")\n",
    "print(f\"   - Max sequence length: {config.max_length}\")\n",
    "print(f\"   - Steps per epoch: {len(train_dataset) // (config.train_batch_size * config.gradient_accumulation_steps)}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "try:\n",
    "    # Train the model\n",
    "    trainer.train()\n",
    "    \n",
    "    training_time = time.time() - start_time\n",
    "    print(f\"\\nâœ… Training completed successfully!\")\n",
    "    print(f\"â±ï¸  Total training time: {training_time / 3600:.2f} hours ({training_time / 60:.1f} minutes)\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Training failed: {str(e)}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    \n",
    "finally:\n",
    "    clear_memory()\n",
    "    check_gpu_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6dd2da",
   "metadata": {},
   "source": [
    "## 7. Model Evaluation and Testing\n",
    "\n",
    "Evaluating model performance and generating sample unit tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af8c9bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final evaluation on test set\n",
    "print(\"ğŸ“Š Running final evaluation on test set...\")\n",
    "\n",
    "eval_results = trainer.evaluate(eval_dataset=test_dataset)\n",
    "\n",
    "print(f\"\\nğŸ“ˆ Final Evaluation Results:\")\n",
    "print(f\"   - Test Loss: {eval_results['eval_loss']:.4f}\")\n",
    "print(f\"   - Test Perplexity: {np.exp(eval_results['eval_loss']):.2f}\")\n",
    "\n",
    "# Save evaluation results\n",
    "with open(f\"{config.output_dir}/eval_results.json\", \"w\") as f:\n",
    "    json.dump(eval_results, f, indent=2)\n",
    "    \n",
    "print(f\"âœ… Evaluation results saved to {config.output_dir}/eval_results.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476fcf11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test model with sample generations\n",
    "def test_model_generation(model, tokenizer, test_data: List[Dict], num_samples: int = 5):\n",
    "    \"\"\"Test the trained model with sample unit test generations\"\"\"\n",
    "    print(f\"ğŸ§ª Testing model with {num_samples} sample generations...\\n\")\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    # Select random test samples\n",
    "    test_samples = np.random.choice(test_data, min(num_samples, len(test_data)), replace=False)\n",
    "    \n",
    "    for i, sample in enumerate(test_samples):\n",
    "        print(f\"{'='*80}\")\n",
    "        print(f\"Test Sample {i+1}/{num_samples}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        # Extract code from sample\n",
    "        if 'code' in sample:\n",
    "            code = sample['code']\n",
    "        else:\n",
    "            # Try to extract from tokenized data\n",
    "            input_ids = sample.get('input_ids', [])\n",
    "            if input_ids:\n",
    "                decoded = tokenizer.decode(input_ids[:200], skip_special_tokens=True)\n",
    "                # Extract code portion (simplified)\n",
    "                code = decoded.split('# Unit test:')[0].replace('# Generate a unit test for the following Python function:', '').strip()\n",
    "            else:\n",
    "                code = \"Sample code not available\"\n",
    "        \n",
    "        print(f\"ğŸ“ Original Code:\")\n",
    "        print(f\"{code[:300]}{'...' if len(code) > 300 else ''}\\n\")\n",
    "        \n",
    "        # Create generation prompt\n",
    "        prompt = f\"# Generate a unit test for the following Python function:\\n{code}\\n\\n# Unit test:\\n\"\n",
    "        \n",
    "        # Tokenize prompt\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "        inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "        \n",
    "        # Generate unit test\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=300,\n",
    "                temperature=0.7,\n",
    "                do_sample=True,\n",
    "                top_p=0.9,\n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "                eos_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "        \n",
    "        # Decode generated response\n",
    "        full_response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        generated_test = full_response[len(prompt):].strip()\n",
    "        \n",
    "        print(f\"ğŸ¤– Generated Unit Test:\")\n",
    "        print(f\"{generated_test}\\n\")\n",
    "        \n",
    "        # Show original unit test if available\n",
    "        if 'unit_test' in sample:\n",
    "            print(f\"âœ… Original Unit Test:\")\n",
    "            print(f\"{sample['unit_test'][:300]}{'...' if len(sample['unit_test']) > 300 else ''}\\n\")\n",
    "        \n",
    "        print(\"\\n\")\n",
    "\n",
    "# Run sample generations\n",
    "test_model_generation(model, tokenizer, test_data, num_samples=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd6bda1",
   "metadata": {},
   "source": [
    "## 8. Save Trained Model\n",
    "\n",
    "Saving the fine-tuned model and creating downloadable archives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63868a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "print(\"ğŸ’¾ Saving trained model...\")\n",
    "\n",
    "# Save model and tokenizer\n",
    "trainer.save_model(config.output_dir)\n",
    "tokenizer.save_pretrained(config.output_dir)\n",
    "\n",
    "print(f\"âœ… Model saved to: {config.output_dir}\")\n",
    "\n",
    "# Save training configuration\n",
    "config_dict = {\n",
    "    'model_name': config.model_name,\n",
    "    'max_length': config.max_length,\n",
    "    'train_batch_size': config.train_batch_size,\n",
    "    'gradient_accumulation_steps': config.gradient_accumulation_steps,\n",
    "    'num_epochs': config.num_epochs,\n",
    "    'learning_rate': config.learning_rate,\n",
    "    'lora_r': config.lora_r,\n",
    "    'lora_alpha': config.lora_alpha,\n",
    "    'lora_dropout': config.lora_dropout,\n",
    "    'target_modules': config.target_modules,\n",
    "    'training_samples': len(train_dataset),\n",
    "    'validation_samples': len(val_dataset),\n",
    "    'test_samples': len(test_dataset)\n",
    "}\n",
    "\n",
    "with open(f\"{config.output_dir}/training_config.json\", \"w\") as f:\n",
    "    json.dump(config_dict, f, indent=2)\n",
    "\n",
    "print(f\"âœ… Training configuration saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e872c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List saved files\n",
    "print(\"ğŸ“ Saved model files:\")\n",
    "for file in os.listdir(config.output_dir):\n",
    "    file_path = os.path.join(config.output_dir, file)\n",
    "    if os.path.isfile(file_path):\n",
    "        size_mb = os.path.getsize(file_path) / 1024 / 1024\n",
    "        print(f\"   - {file}: {size_mb:.1f} MB\")\n",
    "    else:\n",
    "        print(f\"   - {file}/ (directory)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af559eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create downloadable archive\n",
    "print(\"ğŸ“¦ Creating downloadable model archive...\")\n",
    "\n",
    "archive_name = \"qwen-coder-unittest-model\"\n",
    "!cd /kaggle/working && tar -czf {archive_name}.tar.gz qwen-coder-finetune/\n",
    "\n",
    "# Check archive size\n",
    "archive_path = f\"/kaggle/working/{archive_name}.tar.gz\"\n",
    "if os.path.exists(archive_path):\n",
    "    archive_size = os.path.getsize(archive_path) / 1024 / 1024\n",
    "    print(f\"âœ… Model archive created: {archive_name}.tar.gz ({archive_size:.1f} MB)\")\n",
    "    print(f\"ğŸ“¥ Download from: /kaggle/working/{archive_name}.tar.gz\")\n",
    "else:\n",
    "    print(f\"âŒ Failed to create archive\")\n",
    "\n",
    "# Final memory cleanup\n",
    "clear_memory()\n",
    "print(\"\\nğŸ‰ Training pipeline completed successfully!\")\n",
    "print(f\"ğŸ“ Model saved in: {config.output_dir}\")\n",
    "print(f\"ğŸ“¦ Download archive: {archive_name}.tar.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ab514d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training summary\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ğŸ¯ TRAINING SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(f\"ğŸ“Š Dataset: {len(train_data) + len(val_data) + len(test_data):,} total samples\")\n",
    "print(f\"ğŸ¤– Model: {config.model_name}\")\n",
    "print(f\"ğŸ”§ Method: LoRA fine-tuning with 4-bit quantization\")\n",
    "print(f\"âš¡ Hardware: Kaggle P100 GPU\")\n",
    "print(f\"ğŸ“ˆ Training: {config.num_epochs} epochs, {len(train_dataset):,} samples\")\n",
    "print(f\"ğŸ’¾ Output: {config.output_dir}\")\n",
    "print(f\"ğŸ“¦ Archive: {archive_name}.tar.gz\")\n",
    "print(\"\\nâœ… Ready for deployment and inference!\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
